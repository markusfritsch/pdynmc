%\VignetteIndexEntry{pdynmc: An R-package for estimating linear dynamic panel data models based on linear and nonlinear moment conditions}
%\VignetteEngine{R.rsp::tex}
%\VignetteKeyword{R}
%\VignetteKeyword{package}
%\VignetteKeyword{pdynmc}
%\VignetteKeyword{dynamic panel}
%\VignetteKeyword{nonlinear moment conditions}

\documentclass[nojss]{jss}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{nicefrac}
\setcounter{MaxMatrixCols}{30}
\usepackage{arydshln}           %JS: for dashed lines
\usepackage{pdflscape}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% almost as usual
\author{Markus Fritsch\\University of Passau \And
        Andrew Adrian Yu Pua\\ WISE, Xiamen University \And
        Joachim Schnurbus\\University of Passau}
\title{\pkg{pdynmc} -- An \proglang{R}-Package for Estimating Linear Dynamic Panel Data Models Based on Nonlinear Moment Conditions}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Markus Fritsch, Andrew Adrian Pua, Joachim Schnurbus} %% comma-separated
\Plaintitle{pdynmc -- An R-package for estimating linear dynamic panel data models based on quadratic moment conditions} %% without formatting
\Shorttitle{\pkg{pdynmc} -- An R-package employing quadratic moment conditions ...} %% a short title (if necessary)

\Abstract{
\pkg{pdynmc} is an \proglang{R}-package for GMM estimation of linear dynamic panel data models that are based on nonlinear moment conditions as proposed by \citet{AhnSch1995}. In addition, we allow for iterated GMM in light of recent developments in its theory. This paper provides a description of the variety of options regarding instrument type, covariate type, estimation methodology, and general configuration. All functionality is demonstrated through the most popular  firm-level dataset in panel data econometrics \citep[]{AreBon1991} and we relate to other software and packages.
}
\Keywords{dynamic panels, generalized method of moments, iterated GMM, quadratic moment conditions, \proglang{R}}
\Plainkeywords{dynamic panels, generalized method of moments, iterated GMM, quadratic moment conditions, R} %% without formatting
%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Markus Fritsch\\
  Chair of Statistics and Data Analytics\\
  School of Business, Economics, and Information Systems\\
  University of Passau\\
  94032 Passau, Germany\\
  E-mail: \email{markus.fritsch@uni-passau.de}\\
  URL: \url{https://www.wiwi.uni-passau.de/en/statistics/team/assistant-professors/}\\

  Andrew Adrian Yu Pua\\
  MOE Key Laboratory of Econometrics, The Wang Yanan Institute for Studies
in Economics, Department of Statistics, School of Economics, Xiamen
University\\
  Fujian Key Laboratory of Statistical Science, Xiamen University\\
  E-mail: \email{andrewypua@outlook.com}\\
  URL: \url{http://andrew-pua.ghost.io}\\

  Joachim Schnurbus\\
  Computational Statistics and Mathematics Teaching Unit\\
  School of Business, Economics, and Information Systems\\
  University of Passau\\
  94032 Passau, Germany\\
  E-mail: \email{joachim.schnurbus@uni-passau.de}\\
  URL: \url{https://www.wiwi.uni-passau.de/csm/team/}
}
%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/512/507-7103
%% Fax: +43/512/507-2851

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{document}

%% include your article here, just as usual
%% Note that you should use the \texttt{}, \texttt{} and \texttt{} commands.

\section{Introduction}
The formulation of the linear dynamic panel data model accounts for dynamics and unobserved individual-specific heterogeneity simultaneously. Due to the presence of unobserved individual-specific heterogeneity and lagged dependent variables, applying ordinary least squares including individual-specific dummy variables is inconsistent \citep[see, e.g.,][]{Hsi2014}.

A suitable alternative for obtaining parameter estimates in linear dynamic panel data models is deriving moment conditions (or population orthogonality conditions) from the model assumptions. The moment conditions may be linear \citep{AndHsi1982,HolNewRos1988,AreBov1995} or quadratic \citep{AhnSch1995} in parameters and determine the natural instruments available for estimation. Usually, the number of moment conditions exceeds the number of parameters and the moment conditions need to be aggregated appropriately. This can be achieved by the generalized method of moments (GMM), where (weighted) linear combinations of the moment conditions are employed to obtain parameter estimates.

Theoretical results and evidence from Monte Carlo simulations in the literature suggest that incorporating the quadratic moment conditions proposed by \citet{AhnSch1995} may prove valuable for particular data generating processes (DGPs). A relevant example in practice is when the DGP exhibits high persistence and the linear moment conditions fail to identify the model parameters. In this situation, the quadratic moment conditions may still provide identification \citep{BunKle2014,BunSar2015,GorHanXue2016}, %CITE OUR PAPERS HERE
and they are immediate by-products of imposing only the so-called standard assumptions (henceforth SA). Furthermore, we have also been working on other theoretical and practical aspects related to these quadratic moment conditions \citep{PuaFriSch2019a, PuaFriSch2019b}. These SA are the basis of the \cite{AreBon1991} estimator, arguably the most popular default routine in dynamic panel data estimation.

Since the moment conditions employed in GMM estimation of linear dynamic panel data models are derived from model assumptions, a basic understanding of these assumptions is vital for setting up a plausible estimation routine. The methodological part of this paper briefly reviews the assumptions implied when using particular moment conditions in estimation. As a result, we add to the exposition in the \pkg{plm} vignette, where one of its functions \pkg{pgmm} is used to estimate linear dynamic panel data models as well. For further reading on the methodology, we suggest \citet{Fri2019} and the book-length treatment in \citet{CroiMill2018}.

\section{Main contributions}
%\section[Introduction]{Introduction}
%% Note: If there is markup in \(sub)section, then it has to be escape as above.

Even though quadratic moment conditions were proposed more than 20 years ago, standard estimation routines are generally not available across statistical software. To the best of our knowledge, there is currently only the implementation provided by \citep{Kri2019}, called  \pkg{xtdpdgmm}, for the commercial statistical software \proglang{Stata} \citep{Sta2015} that is explicitly designed to incorporate nonlinear moment conditions into GMM estimation. Furthermore, the current implementation in Stata requires version 13 or higher.

Our package \pkg{pdynmc} provides an implementation of GMM estimation of linear dynamic panel data models based on \citet{HolNewRos1988}, \citet{AreBov1995}, and \citet{AhnSch1995} moment conditions in the open source statistical software \proglang{R} \citep{RCo2019}.
In contrast to routines in the existing packages \pkg{plm} \citep{CroMil2008}, and \pkg{panelvar} \citep{SigFer2018panelvar}, we allow the inclusion of quadratic moment conditions into the analysis. Nevertheless, the package \pkg{panelvar} allows the user to perform lag selection based on information criteria and structural analysis based on impulse response functions, while the package \pkg{plm} provides a variety of functions for the estimation of static and dynamic linear panel models, along with some random effects estimators and a number of different specification tests. Yet another package that tries to exploit the panel data structure by generalized estimating equations is \pkg{panelr} \citep{panelr}.

Furthermore, just like the function \pkg{pgmm} in the \pkg{plm} package, \pkg{pdynmc} allows the computation of one-step and two-step closed form GMM estimators and standard specification testing such as overidentifying restrictions tests, serial correlation tests, and Wald tests. These features are shared by other packages implemented in \proglang{Gauss}, \proglang{Ox} \citep{DooAreBon2012dpd}, \proglang{R}, and \proglang{Stata}.


More importantly, our contributed package already includes iterated GMM estimation as a default. Recent work by \citet{HansenLee2019} have shown the merits of the iterated GMM estimator and develop the theory under potential misspecification of moment conditions. Unfortunately, we have not incorporated their theory into the package but would eventually incorporate the theory to enable other researchers to use our package for simulation studies. For the moment, this feature is still "bleeding-edge" for the general audience. Furthermore, having iterated GMM for dynamic panels available may help in further work to extend results found in, for example, \citet{HWANG2018381}.

Regrettably, \pkg{pdynmc} is not as fast as \pkg{xtdpdgmm}. Some alternative \proglang{R} codes that have substantial speed improvements are available from \citet{PHILLIPS2019119} but lack the functionality of a full package. As a check of the current functionality of \pkg{pdynmc}, we estimated the model they used as a DGP for their simulations. We simulated only one realization of their DGP, which is a linear AR(1) dynamic panel model with one exogenous covariate with 1000 cross-sectional units, 40 time periods. We were able to estimate the model for one realization of their DGP in 1.5 to 2 minutes. In contrast, \pkg{pgmm} fails to even start. Although our computational time does not match \citet{PHILLIPS2019119},  we believe this is a good start and will continue working on speed improvements of \pkg{pdynmc} in future releases of the package.

The structure of the paper is as follows. Section \ref{sec:ldpdm} briefly sketches the linear dynamic panel data model, states the underlying assumptions frequently used in the literature, and describes the moment conditions arising from the model assumptions. Section \ref{sec:gmm} covers GMM estimation of linear dynamic panel data models and illustrates the minimization criterion, and estimation in one, two, or multiple steps.
Section 5 outlines the computation of standard errors, specification testing and overidentifying restrictions testing, and the testing of general linear hypotheses.
Section \ref{sec:example} illustrates the estimation of linear dynamic panel data models with \pkg{pdynmc} for the data set of \citet{AreBon1991} on adjustments of employment of firms located in the United Kingdom.
Section \ref{sec:conclusion} concludes and sketches functionality we plan to add to future releases of the package.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Linear dynamic panel data model} \label{sec:ldpdm}
\subsection{Model and standard assumptions}
For a given dataset with cross section dimension $n$ and time series dimension $T$, consider a typical linear dynamic panel data model of the following form:
\begin{align}
y_{i, t} & = \alpha y_{i, t - 1} + \beta x_{i, t} + u_{i,t}, \quad i = 1, \dots, n;\ t = 2, \dots, T,\label{EQ00-1:lin-dyn-pdm} \\
u_{i,t} & = \eta_{i} + \varepsilon_{i, t}. \label{EQ00-2:lin-dyn-pdm}
\end{align}
Here $y_{i,t}$ and $y_{i,t-1}$ denote the dependent variable and its lag, $\alpha$ is the lag parameter, and $x_{i,t}$ is a single covariate with corresponding slope coefficient $\beta$. The second equation shows that the model allows for a composite error term $u_{i,t}$ which can be separated into an unobserved individual-specific effect $\eta_i$ and an idiosyncratic remainder component $\varepsilon_{i,t}$.

Combining the Equations (\ref{EQ00-1:lin-dyn-pdm}) and (\ref{EQ00-2:lin-dyn-pdm}) yields the single equation form of the model
\begin{align}
y_{i, t} & = \alpha y_{i, t - 1} + \beta x_{i, t} + \eta_{i} + \varepsilon_{i, t}, \quad i = 1, \dots, n;\ t = 2, \dots, T \label{EQ01:lin-dyn-pdm}.
\end{align}

For illustration purposes in this section, we only include one lag of the dependent variable, one covariate, and omit unobserved time-specific effects for simplicity of exposition and notational convenience. Extending the representation is straightforward and \pkg{pdynmc} can also accommodate AR(p) models. The initial time period is denoted by $t=1$.

In the linear dynamic panel data literature, researchers have focsued on the following standard assumptions, henceforth SA, \citep[see][]{AhnSch1995}:
\begin{align} \label{EQ02:StdAssumpt}
& \text{The data are independently distributed only across} \ i, \\
& E(\eta_i) = 0, \quad i=1,...,n, \nonumber \\
& E(\varepsilon_{i,t}) = 0, \quad i=1,...,n;\ t=2,...,T, \nonumber \\
& E(\varepsilon_{i,t} \cdot \eta_i) = 0, \quad i=1,...,n;\ t=2,...,T, \nonumber \\
& E(\varepsilon_{i,t} \cdot \varepsilon_{i,s}) = 0, \quad i=1,...,n;\ t \neq s, \nonumber \\
& E(y_{i,1} \cdot \varepsilon_{i,t}) = 0, \quad i=1,...,n;\ t=2,...,T, \nonumber \\
%& \alpha \in \left[-1,1\right] \setminus{\{0\}}, \nonumber \\
& n \rightarrow \infty, \ \text{while}\ T \  \text{is fixed, such that}\ \frac{T}{n} \rightarrow 0. \nonumber
\end{align}
We will assume that the SA holds for the rest of the paper. Next, we discuss how these SA help in the estimation of the parameters of the linear dynamic panel data model.


\subsection{Moment conditions from standard assumptions} \label{sec:StdAssumpt}

The unobserved individual-specific effects $\eta_i$ may be eliminated from Equation (\ref{EQ01:lin-dyn-pdm}) by taking first differences, i.e.,
\begin{align} \label{EQ03:FDlin-dyn-pdm}
\Delta y_{i, t} = \alpha \Delta y_{i, t - 1} + \beta \Delta x_{i, t} + \Delta \varepsilon_{i, t}, \quad i = 1, \dots, n;\ t = 2, \dots, T.
\end{align}
Because the first difference of the lagged dependent variable $\Delta y_{i,t-1} = y_{i,t-1} - y_{i,t-2}$ and the first difference of the idiosyncratic remainder component $\Delta \varepsilon_{i,t} = \varepsilon_{i,t} - \varepsilon_{i,t-1}$ are not orthogonal, using least squares to estimate Equation (\ref{EQ03:FDlin-dyn-pdm}) produces inconsistent estimators. The SA stated in Equation (\ref{EQ02:StdAssumpt}) provide a remedy.


In particular, \citet{HolNewRos1988} (henceforth HNR) propose linear (in parameters) moment conditions
\begin{align}\label{EQ04:HNR-MC-linear}
E(y_{i,s} \cdot \Delta u_{i,t}) = 0, \qquad t = 3,\dots,T;\ s = 1,\dots,t - 2.
\end{align}
Equation (\ref{EQ04:HNR-MC-linear}) provides $0.5(T-1)(T-2)$ moment conditions.
Similar moment conditions can be derived from the covariate $x_{i,t}$, depending on its exogeneity classification. Endogenous, predetermined, and (strictly) exogenous covariates provide the following linear moment conditions, respectively:
\begin{align} \label{EQ06:HNR-MC-linear-x_it}
E(x_{i,s} \cdot \Delta u_{i,t}) = 0, \qquad t = 3,\dots,T, \qquad &\text{where}& \\
s = 1,\dots,t - 2, \qquad &\text{for} \qquad x \ \  \text{endogenous}, \nonumber \\
s = 1,\dots,t - 1, \qquad &\text{for} \qquad x \ \ \text{predetermined}, \nonumber \\
s = 1,\dots,T,     \qquad &\text{for} \qquad x \ \ \text{strictly exogenous}. \nonumber
\end{align}

\citet{AhnSch1995} (henceforth AS) introduced the following $T-3$ additional moment conditions, which are also implied by SA in Equation (\ref{EQ02:StdAssumpt}):
\begin{align}\label{EQ05:AS-MC-nonlinear}
E(u_{i,t} \cdot \Delta u_{i,t-1}) = 0, \qquad t = 4,\dots,T.
\end{align}
Rewriting the equation and expressing the moment conditions in terms of parameters and observables reveals that the AS moment conditions are quadratic in parameters.

Parameter estimates can be obtained by stacking together the sample analogues of the previously discussed moment conditions.
As a result, we have $m$ sample moment conditions $\overline{\mathbf{M}} = \frac{1}{n} \sum_{i = 1}^n \mathbf{M}_i$. Now, suppose $x_{it}$ is a predetermined covariate and we want to use all the corresponding HNR moment conditions in (\ref{EQ04:HNR-MC-linear}) and (\ref{EQ06:HNR-MC-linear-x_it}), and the AS moment conditions in (\ref{EQ05:AS-MC-nonlinear}). Now, consider the following moment conditions on the left panel and the corresponding vector of individual moment condition contributions on the right panel $\mathbf{M}_i$ to be available for estimation:

\begin{footnotesize}
\begin{align}
\underbrace{\left(
                 \begin{array}{c}
E(y_{i,1} \cdot \Delta u_{i,3}) \\
E(y_{i,1} \cdot \Delta u_{i,4}) \\
E(y_{i,2} \cdot \Delta u_{i,4}) \\
E(y_{i,1} \cdot \Delta u_{i,5}) \\
\vdots \\
E(y_{i,3} \cdot \Delta u_{i,5}) \\
\vdots \\
E(y_{i,T - 2} \cdot \Delta u_{i,T}) \\
\hdashline
E(x_{i,1} \cdot \Delta u_{i,3}) \\
E(x_{i,2} \cdot \Delta u_{i,3}) \\
E(x_{i,1} \cdot \Delta u_{i,4}) \\
\vdots \\
E(x_{i,3} \cdot \Delta u_{i,4}) \\
\vdots \\
E(x_{i,T - 1} \cdot \Delta u_{i,T}) \\
\hdashline
E(u_{i,4} \cdot \Delta u_{i,3}) \\
\vdots \\
E(u_{i,T} \cdot \Delta u_{i,T - 1}) \\
                 \end{array}
               \right)}_{m \times 1}
=
\left(
                 \begin{array}{c}
0 \\
0 \\
\\
\\
\vdots \\
\\
\\
0 \\
\hdashline
0 \\
\\
\\
\vdots \\
\\
\\
0 \\
\hdashline
0 \\
\vdots \\
0 \\
                 \end{array}
               \right),
\hspace{2cm} \underbrace{\mathbf{M}_i}_{m \times 1}  = \left(
                 \begin{array}{c}
y_{i, 1} \cdot \widetilde{\Delta u}_{i, 3} \\
y_{i, 1} \cdot \widetilde{\Delta u}_{i, 4} \\
y_{i, 2} \cdot \widetilde{\Delta u}_{i, 4} \\
y_{i, 1} \cdot \widetilde{\Delta u}_{i, 5} \\
\vdots \\
y_{i, 3} \cdot \widetilde{\Delta u}_{i, 5} \\
\vdots \\
y_{i, T - 2} \cdot \widetilde{\Delta u}_{i, T} \\
\hdashline
x_{i, 1} \cdot \widetilde{\Delta u}_{i, 3} \\
x_{i, 2} \cdot \widetilde{\Delta u}_{i, 3} \\
x_{i, 1} \cdot \widetilde{\Delta u}_{i, 4} \\
\vdots \\
x_{i, 3} \cdot \widetilde{\Delta u}_{i, 4} \\
\vdots \\
x_{i, T - 1} \cdot \widetilde{\Delta u}_{i, T} \\
\hdashline
\widetilde{u}_{i, 4} \cdot \widetilde{\Delta u}_{i, 3} \\
\vdots \\
\widetilde{u}_{i, T} \cdot \widetilde{\Delta u}_{i, T - 1} \\
                 \end{array}
               \right). \label{two-panel}
\end{align}
\end{footnotesize}

The dashed lines separate the three different sets of moment conditions. Note that we also distinguish the unobservable $u_{i,t}$ and $\Delta u_{i,t}$ from what can be computed from the data, namely $\widetilde{u}_{i,t}$ and $\widetilde{\Delta u}_{i,t}$, in both panels of (\ref{two-panel}) for emphasis. Further consider decomposing the individual moment condition contributions (seen in the right panel of (\ref{two-panel})) into $\boldsymbol{M}_i = \boldsymbol{Z}_i' \cdot \tilde{\boldsymbol{s}}_i$, where $\boldsymbol{Z}_i'$ denotes the transpose of a matrix that does not depend on parameter estimates, while the column vector $\tilde{\boldsymbol{s}}_i$ does. From the right panel of (\ref{two-panel}), we have:

\begin{footnotesize}
\begin{align*}
\boldsymbol{Z}_i' = \underbrace{\left(
                  \begin{array}{cccc:cccc}
y_{i, 1} & 0        &   \cdots  & 0            &   0      & \cdots   &           &   0    \\
0        & y_{i,1}  &           &              &          &          &           &       \\
         & y_{i,2}  &           &              &          &          &           &       \\
         & 0        &           &              &          &          &           &       \\
\vdots   & \vdots   &           & \vdots       & \vdots   &          &           & \vdots  \\
         &          &           &              &          &          &           &       \\
         &          &           & 0            &          &          &           &       \\
         &          &           & y_{i,1}      &          &          &           &       \\
         &          &           & \vdots       &          &          &           &       \\
0        & 0        &           & y_{i,T-2}    &   0      & \cdots   &           &   0    \\
\hdashline
x_{i,1}  & 0        &   \cdots  & 0            &   0      & \cdots   &           &   0    \\
x_{i,2}  & 0        &           &              &          &          &           &       \\
0        & x_{i,1}  &           &              &          &          &           &       \\
         & x_{i,2}  &           &              &          &          &           &  \\
         & x_{i,3}  &           &              &          &          &           &       \\
         & 0        &           &              &          &          &           &       \\
\vdots   & \vdots   &           & \vdots       & \vdots   &          &           & \vdots  \\
         &          &           &              &          &          &           &       \\
         &          &           & 0            &          &          &           &       \\
         &          &           & x_{i,1}      &          &          &           &       \\
         &          &           & \vdots       &          &          &           &       \\
0        & 0        &           & x_{i, T - 1} &   0      & \cdots   &           &   0    \\
\hdashline
0        &          &   \cdots  & 0            & 1        &  0       & \cdots    &   0 \\
\vdots   &          &           & \vdots       & \vdots   & \ddots   &           &   \vdots \\
         &          &           &              &          &          &           &   0 \\
0        &          &   \cdots  & 0            & 0        & \cdots   &  0        &   1 \\
                  \end{array}
                \right)}_{m \times (2T - 5)}
, \hspace{0.5cm} \tilde{\boldsymbol{s}}_i = \underbrace{\left(
                                        \begin{array}{c}
                                          \widetilde{\Delta u}_{i, 3} \\
                                          \widetilde{\Delta u}_{i, 4} \\
                                          \vdots \\
                                          \widetilde{\Delta u}_{i, T} \\
                                          \hdashline
                                          \widetilde{u}_{i, 4} \cdot \widetilde{\Delta u}_{i, 3} \\
                                          \widetilde{u}_{i, 5} \cdot \widetilde{\Delta u}_{i, 4} \\
                                          \vdots \\
                                          \widetilde{u}_{i, T} \cdot \widetilde{\Delta u}_{i, T - 1} \\
                                        \end{array}
                                      \right)}_{(2T - 5) \times 1}.
\end{align*}
\end{footnotesize}



\subsection{Moment conditions from extended assumptions} \label{sec:ExtAssumpt}
Another set of moment conditions beyond those implied by SA that is popular in theoretical and applied research are those conditions that may be derived from the assumption that
\begin{equation} \label{EQ07:CCE}
E(\Delta y_{i,t} \cdot \eta_i) = 0, \quad i=1,\dots,n. %\label{mean-stat}
\end{equation}
This expression requires that the dependent variable and the unobserved individual-specific effects are constantly correlated over time for each individual. Deviations from the assumption are required to be unsystematic over both, the cross section and the time series dimension \citep[see Section 6.5 in][which also provides an empirically relevant example]{Are2003}.
\citet{BluBonWin2001} state that if $\Delta y_{i,t}$ and $\eta_i$ are uncorrelated, then it places restrictions on the relationship between $\Delta x_{i,t}$ and $\eta_i$. The latter is problematic from the perspective of the SA because researchers prefer not to restrict a priori the relationship between $x_{i,t}$ and $\eta_i$ -- which is the essence of the fixed-effects approach in dynamic panel data settings.

The assumption in (\ref{EQ07:CCE}) has been called `constant correlated effects' by \citet{BunSar2015}, effect stationarity by \citep[][]{Kiv2007a}, or mean stationarity by \citep[][]{Are2003}.
From this assumption, we can derive $T-2$ additional  \citet{AreBov1995} (henceforth ABov):
\begin{align} \label{EQ08:AB-MC-linear_yit}
E(\Delta y_{i,t-1} \cdot u_{i,t}) = 0, \quad t=3,\dots,T.
\end{align}
By rewriting these moment conditions, it can be shown that the ABov moment conditions encompass the nonlinear AS moment conditions and render the latter redundant for estimation \citep[for a derivation see][]{Fri2019}.

Additional ABov moment conditions can be derived from the covariate $x_{i,t}$, depending on the nature of the $x_{i,t}$:
\begin{align*}
E(\Delta x_{i,v} \cdot u_{i,t}) = 0, \qquad & \text{where} \\
& v = t-1;\ t=3, \dots T, \quad\text{for} \quad x \ \  \text{endogenous}, \\
& v = t;\ t=2,\dots,T, \quad \text{for} \quad x \ \ \text{strictly exogenous} \quad \text{or} \quad x \ \ \text{predetermined}.
\end{align*}
Notice that no additional ABov moment conditions arise if $x_{i,t}$ is strictly exogenous.
When using the HNR and ABov moment conditions to estimate the linear dynamic panel data model in Equation (\ref{EQ01:lin-dyn-pdm}) with a predetermined explanatory variable, $\boldsymbol{M}_i$ is as follows:

\begin{footnotesize}
\begin{align*}
\boldsymbol{Z}_i' & = \underbrace{\left(
                  \begin{array}{cccc:cccc:cccc}
y_{i, 1} & 0        & \cdots   & 0            &   0             & \cdots          &           & 0                   &  0              & \cdots          &        & 0               \\
0        & y_{i, 1} &          &              &                 &                 &           &                     &                 &                 &        &                 \\
         & y_{i, 2} &          &              &                 &                 &           &                     &                 &                 &        &                 \\
\vdots   & 0        &          & \vdots       & \vdots          &                 &           & \vdots              & \vdots          &                 &        & \vdots          \\
         &          &          &              &                 &                 &           &                     &                 &                 &        &                 \\
         &          &          & 0            &                 &                 &           &                     &                 &                 &        &                 \\
         &          &          & y_{i,1}      &                 &                 &           &                     &                 &                 &        &                 \\
         &          &          & \vdots       &                 &                 &           &                     &                 &                 &        &                 \\
0        & \cdots   & 0        & y_{i, T - 2} &   0             & \cdots          &           & 0                   & 0               & \cdots          &        & 0               \\
\hdashline
x_{i, 1} &   0      & \cdots   & 0            &   0             & \cdots          &           & 0                   & 0               & \cdots          &        & 0               \\
x_{i, 2} &   0      &          &              &                 &                 &           &                     &                 &                 &        &                 \\
0        & x_{i, 1} &          &              &                 &                 &           &                     &                 &                 &        &                 \\
         & x_{i, 2} &          &              &                 &                 &           &                     &                 &                 &        &                 \\
         & x_{i, 3} &          &              &                 &                 &           &                     &                 &                 &        &                 \\
\vdots   &   0      &          & \vdots       & \vdots          &                 &           & \vdots              & \vdots          &                 &        & \vdots          \\
         &          &          &              &                 &                 &           &                     &                 &                 &        &                 \\
         &          &          & 0            &                 &                 &           &                     &                 &                 &        &                 \\
         &          &          & x_{i,1}      &                 &                 &           &                     &                 &                 &        &                 \\
         &          &          & \vdots       &                 &                 &           &                     &                 &                 &        &                 \\
0        & \cdots   & 0        & x_{i, T - 1} &   0             & \cdots          &           & 0                   & 0               & \cdots          &        & 0               \\
\hdashline
0        &          & \cdots   & 0            & \Delta y_{i, 2} & 0               & \cdots    & 0                   & 0               & \cdots          &        & 0               \\
\vdots   &          &          & \vdots       & 0               & \Delta y_{i, 3} &           & \vdots              & \vdots          &                 &        & \vdots          \\
         &          &          &              & \vdots          &                 & \ddots    & 0                   &                 &                 &        &                 \\
0        &          & \cdots   & 0            & 0               & \cdots          & 0         & \Delta y_{i, T - 1} & 0               & \cdots          &        & 0               \\
\hdashline
0        &          & \cdots   & 0            & 0               & \cdots          &           & 0                   & \Delta x_{i, 2} & 0               & \cdots & 0               \\
         &          &          &              &                 &                 &           &                     & 0               & \Delta x_{i, 3} &        & \vdots          \\
\vdots   &          &          & \vdots       & \vdots          &                 &           & \vdots              & \vdots          &                 & \ddots & 0               \\
0        &          & \cdots   & 0            & 0               & \cdots          &           & 0                   & 0               & \cdots          & 0      & \Delta x_{i, T} \\
                  \end{array}
                \right)}_{m \times (3T-5)}
,\\
\\
\tilde{\boldsymbol{s}}_i' & = \underbrace{\begin{array}{cccc:cccc:cccc}
(\widetilde{\Delta u}_{i, 3}, & \widetilde{\Delta u}_{i, 4}, & \cdots, & \widetilde{\Delta u}_{i, T} & \widetilde{u}_{i, 3}, & \widetilde{u}_{i, 4}, & \cdots, & \widetilde{u}_{i, T}, & \widetilde{u}_{i, 2}, & \widetilde{u}_{i, 3}, & \cdots, & \widetilde{u}_{i, T})
                            \end{array} }_{1 \times (3T-5)}.
\end{align*}
\end{footnotesize}












%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{GMM estimation} \label{sec:gmm}

\subsection{Minimization criterion}
For a given sample, GMM estimation minimizes the aggregated squared distance of the sample moment conditions from zero and can be represented as
\begin{align}\label{EQ09:Objective-function-GMM}
L_{\boldsymbol{W}}^{2} = \overline{\boldsymbol{M}}' \cdot \boldsymbol{W} \cdot \overline{\boldsymbol{M}}.
\end{align}
The index of the $L_{\boldsymbol{W}}^{2}$-norm expresses that the norm depends on the weighting matrix $\boldsymbol{W}$ and the superscript indicates that the norm is a quadratic form. The $m \times m$ weighting matrix $\boldsymbol{W}$ guides the aggregation of the $m$ sample moment conditions.

Stacking the $\boldsymbol{Z}_i'$ for all cross sectional observations horizontally yields  $\boldsymbol{Z}' = (\boldsymbol{Z}_1', \dots, \boldsymbol{Z}_n')$ and concatenating the column vectors $\tilde{\boldsymbol{s}}_i$ yields the vector $\tilde{\boldsymbol{s}}$. From the notation developed in Section \ref{sec:ldpdm}, where the sample moment conditions are decomposed into a vector $\tilde{\boldsymbol{s}}$ that depends on the parameter estimates and a matrix $\boldsymbol{Z}$ that does not. Plugging these two terms into Equation (\ref{EQ09:Objective-function-GMM}) gives:
\begin{align*}
L_{\boldsymbol{W}}^{2} = \frac{1}{n^2} \cdot \tilde{\boldsymbol{s}}' \boldsymbol{Z} \cdot \boldsymbol{W} \cdot \boldsymbol{Z}' \tilde{\boldsymbol{s}}.
\end{align*}
Minimizing $L^2_{\boldsymbol{W}}$ yields a GMM estimator $\widetilde{\boldsymbol{\theta}}$ which depends on a choice of $\boldsymbol{W}$. When estimating the linear dynamic panel data model in Equation (\ref{EQ01:lin-dyn-pdm}) by GMM based on linear moment conditions only, numerical optimization is not required because closed form linear estimators are available and are programmed in \pkg{pdynmc}. In this situation, the closed form results are computed and stored along with the optimization results.

When nonlinear moment conditions are used, nonlinear optimization techniques are required to obtain coefficient estimates. By default, GMM estimation by \pkg{pdynmc} is based on numerical optimization. To initialize the optimization procedure, starting values are drawn for all parameter estimates from a uniform distribution over the interval $[-1, 1]$.

For the optimization procedure, we rely on the \proglang{R}-package \pkg{optimx} \citep{NasVar2011,Nas2014}. All optimization routines implemented in \pkg{optimx} are available in \pkg{pdynmc}. Based on our experience, the Variable Metric method \citep{Fle1970new,Nas1990compact} seems to work satisfactorily in the estimation of linear dynamic panel data models.
The Variable Metric method is named \code{BFGS} in \pkg{optimx}
and serves as the default procedure in \pkg{pdynmc}. In contrast, the numerical optimization of the GMM objective function used in \pkg{xtdpdgmm} is based on a Gauss-Newton technique.

\subsection{One-step, two-step, and iterated GMM estimation} \label{section:1_2_step}
In practice, GMM estimation is frequently carried out iteratively. In order to start the estimation process, an initial estimate of the weighting matrix $\widehat{\boldsymbol{W}}$ is required. Obviously, plugging in different weighting matrices into Equation (\ref{EQ09:Objective-function-GMM}) yields varying objective function values and different estimates for the model parameters.

Different proposals for the initial weighting matrix -- with varying degrees of asymptotic efficiency -- exist in the literature \citep[see][]{BluBonWin2001} for the various types of moment conditions which can be employed in the estimation of the linear dynamic panel data model in Equation (\ref{EQ01:lin-dyn-pdm}). Common examples involve identity or tridiagonal matrices. Generally, the proposed weighting matrices are based on the expected variances and covariances of the moment conditions and are derived from the underlying model assumptions.
The optimal $\boldsymbol{W}$ is proportional (up to a multiplicative constant) to the inverse of the covariance matrix of the moment conditions \citep[see, e.g.,][]{Are2003}.

A popular proposal for the initial weighting matrix $\widehat{\boldsymbol{W}}_1$ of the one-step GMM estimator (GMM1S) is
\begin{align} \label{EQ10:W1step}
\widehat{\boldsymbol{W}}_1 & = \left(\frac{1}{n} \cdot \boldsymbol{Z}' \boldsymbol{H} \boldsymbol{Z}\right)^{-1}.
\end{align}
The structure of the matrix $\boldsymbol{H}$ varies depending on the types of moment conditions employed in estimation. When only the HNR moment conditions are used, \citet{AreBon1991} propose to set the matrix to

\begin{align*}
\boldsymbol{H}_{HNR} & = \left(
\begin{array}{cccccc}
2       &   -1      &   0       &   0       &   \dots   &   0       \\
-1      &   2       &   -1      &   0       &           &           \\
0       &   -1      &   \ddots  &   \ddots  &   \ddots  &   \vdots  \\
\vdots  &   \ddots  &   \ddots  &           &           &   0       \\
        &           &           &           &   2       &   -1      \\
0       &           &   \dots   &   0       &   -1      &   2
\end{array}
\right).
\end{align*}

The tridiagonal matrix $\boldsymbol{H}_{HNR}$ accounts for the serial correlation in the idiosyncratic remainder components induced after taking first differences of Equation (\ref{EQ01:lin-dyn-pdm}) to eliminate $\eta_i$.

When using only the ABov moment conditions in estimation, a choice for $\boldsymbol{H}$ often encountered in practice is the identity matrix with $T-2$ diagonal elements, i.e. $\boldsymbol{H}_{ABov}=I_{T-2}$. For the AS moment conditions,  $\boldsymbol{H}_{AS}=I_{T-2}$ \citep[see, e.g.,][]{BluBonWin2001,Kri2019} is the default choice. Finally, when two different sets of moment conditions are employed, a general representation of $\boldsymbol{H}$ is
\begin{align*}
\boldsymbol{H} & = \left(
\begin{array}{cc}
\boldsymbol{A}      & \boldsymbol{B}    \\
\boldsymbol{B}'      & \boldsymbol{C}    \\
\end{array}
 \right),
\end{align*}
where the matrices $\boldsymbol{A}$, $\boldsymbol{B}$, and $\boldsymbol{C}$ are chosen depending on the particular moment conditions employed in GMM estimation.


When only linear moment conditions are used, we can obtain a closed form for GMM1S $\hat{\boldsymbol{\theta}}_{1}$ as:
\begin{align} \label{EQ12:closedForm1step}
\hat{\boldsymbol{\theta}}_{1} = (\boldsymbol{X}' \boldsymbol{Z} \widehat{\boldsymbol{W}}_{1} \boldsymbol{Z}' \boldsymbol{X})^{-1} \boldsymbol{X}' \boldsymbol{Z} \widehat{\boldsymbol{W}}_{1} \boldsymbol{Z}' \boldsymbol{y}.
\end{align}
The matrix $\boldsymbol{X}$ contains all the first differences of the right-hand side variables, along with the corresponding levels depending on the set of moment conditions used. Similarly, $\boldsymbol{y}$ contains all the first differences of the left-hand side variables, along with the corresponding levels.

An estimate for the weighting matrix $\widehat{\boldsymbol{W}}_2$ of the two-step GMM estimator (GMM2S) is
\begin{align} \label{EQ11:W2step}
\widehat{\boldsymbol{W}}_2 & = \left(\frac{1}{n} \cdot \boldsymbol{Z}' \hat{\boldsymbol{s}}_{1} \hat{\boldsymbol{s}}_{1}' \boldsymbol{Z}\right)^{-1},
\end{align}
where $\hat{\boldsymbol{s}}_{1}$ denotes the residuals from one-step estimation.  In order to calculate the two-step coefficient estimates $\hat{\boldsymbol{\theta}}_{2}$, $\widehat{\boldsymbol{W}}_{1}$ needs to be replaced by the estimated two-step weighting matrix $\widehat{\boldsymbol{W}}_{2}$:
\begin{align} \label{EQ13:closedForm2step}
\hat{\boldsymbol{\theta}}_{2} = (\boldsymbol{X}' \boldsymbol{Z} \widehat{\boldsymbol{W}}_{2} \boldsymbol{Z}' \boldsymbol{X})^{-1} \boldsymbol{X}' \boldsymbol{Z} \widehat{\boldsymbol{W}}_{2} \boldsymbol{Z}' \boldsymbol{y}.
\end{align}

We can continue  by obtaining residuals from two-step estimation,  computing the third-step weighting matrix using those residuals, and obtaining the third-step GMM estimator. If we update the residuals until either the change in coefficient estimates from one estimation step to the next does not exceed a certain pre-specified threshold $z_{\text{tol}}$ or after a pre-specified number of maximum iterations $h_{\text{iter}}$, then the result is an iterated GMM estimator.



\section{Standard errors and inference}

\subsection{Standard errors} \label{sec:covariance_matrix}
Asymptotic one-step standard errors for the estimated coefficients can be obtained by taking the square root of the main diagonal elements of the estimated one-step variance covariance matrix
\begin{align} \label{EQ14:VCov_beta_1step}
\widehat{\boldsymbol{\Omega}}(\hat{\boldsymbol{\theta}}_{1}) = n \cdot (\boldsymbol{X}' \boldsymbol{Z} \widehat{\boldsymbol{W}}_{1} \boldsymbol{Z}' \boldsymbol{X})^{-1} \hat{\sigma}_{1}^2, \qquad \text{with} \qquad \hat{\sigma}_{1}^2 = \hat{\boldsymbol{s}}_{1}'\hat{\boldsymbol{s}}_{1} \cdot \frac{1}{N - p}.
\end{align}
In the formula, $N$ is the number of observations available for estimation (i.e., $n$ times $T$ minus the number of missing observations), $p$ denotes the number of estimated coefficients, and $\hat{\boldsymbol{s}}_{1}$ are residuals from GMM1S \citep[see][]{DooAreBon2012dpd}. As stated in \citet{Win2005}, robust one-step standard errors are available from
\begin{align} \label{EQ15:VCov_beta_1step_rob}
\widehat{\boldsymbol{\Omega}}_{r}(\hat{\boldsymbol{\theta}}_{1}) = n \cdot & (\boldsymbol{X}' \boldsymbol{Z} \widehat{\boldsymbol{W}}_{1} \boldsymbol{Z}' \boldsymbol{X})^{-1} \boldsymbol{X}' \boldsymbol{Z} \widehat{\boldsymbol{W}}_{1} \widehat{\boldsymbol{W}}_{2}^{-1} \widehat{\boldsymbol{W}}_{1} \boldsymbol{Z}' \boldsymbol{X} (\boldsymbol{X}' \boldsymbol{Z} \widehat{\boldsymbol{W}}_{1} \boldsymbol{Z}' \boldsymbol{X})^{-1},
\end{align}
while asymptotic two-step standard errors can be computed from
\begin{align} \label{EQ16:VCov_beta_2step}
\widehat{\boldsymbol{\Omega}}(\hat{\boldsymbol{\theta}}_{2}) = n \cdot (\boldsymbol{X}' \boldsymbol{Z} \widehat{\boldsymbol{W}}_{2} \boldsymbol{Z}' \boldsymbol{X})^{-1}.
\end{align}
Since asymptotic two-step GMM standard errors for the estimated coefficients exhibit a downward bias in small samples, they can, however, be substantially lower than one-step GMM standard errors \citep[see, e.g.,][]{AreBon1991}. \citet{Win2005} relates the bias to the dependence of the two-step weighting matrix on parameter estimates (the one-step estimates) and proposes an analytic correction of the two-step standard errors based on a first order Taylor-series expansion:
\begin{align} \label{EQ17:WindmeijerCorrSE}
\widehat{\boldsymbol{\Omega}}_c (\hat{\boldsymbol{\theta}}_{2}) = & \widehat{\boldsymbol{\Omega}}(\hat{\boldsymbol{\theta}}_{2}) + \boldsymbol{D}_{\hat{\boldsymbol{\theta}}_{2}, \widehat{\boldsymbol{W}}_{2}} \widehat{\boldsymbol{\Omega}}(\hat{\boldsymbol{\theta}}_{2}) + \widehat{\boldsymbol{\Omega}}(\hat{\boldsymbol{\theta}}_{2}) \boldsymbol{D}_{\hat{\boldsymbol{\theta}}_{2}, \widehat{\boldsymbol{W}}_{2}}' \\
& + \boldsymbol{D}_{\hat{\boldsymbol{\theta}}_{2}, \widehat{\boldsymbol{W}}_{2}} \widehat{\boldsymbol{\Omega}}_r(\hat{\boldsymbol{\theta}}_{1}) \boldsymbol{D}_{\hat{\boldsymbol{\theta}}_{2}, \widehat{\boldsymbol{W}}_{2}}', \nonumber
\end{align}
Notice that the first term is the estimated uncorrected two-step variance covariance matrix of the coefficient estimates.
The computation of the correction $\boldsymbol{D}_{\hat{\boldsymbol{\theta}}_{2}, \widehat{\boldsymbol{W}}_{2}}$ is involved when multiple parameters are estimated. For a single parameter, it equals
\begin{align*}
\boldsymbol{D}_{\hat{\boldsymbol{\theta}}_{2}, \widehat{\boldsymbol{W}}_{2}} = - \frac{1}{n} \cdot \widehat{\boldsymbol{\Omega}}(\hat{\boldsymbol{\theta}}_{2}) \boldsymbol{X}' \boldsymbol{Z} \widehat{\boldsymbol{W}}_{2} \left. \frac{\partial \widehat{\boldsymbol{W}}^{-1}(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \right\rvert_{\boldsymbol{\theta} = \hat{\boldsymbol{\theta}}_{1}} \widehat{\boldsymbol{W}}_{2} \boldsymbol{Z}' \hat{\boldsymbol{s}}_{2}.
\end{align*}
The vector $\hat{\boldsymbol{s}}_{2}$ denotes the two-step residuals and the first derivative of the weighting matrix for two-step GMM estimation evaluated at $\hat{\boldsymbol{\theta}}_{1}$ can be calculated from
\begin{align*}
\left. \frac{\partial \widehat{\boldsymbol{W}}^{-1}(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \right\rvert_{\boldsymbol{\theta} = \hat{\boldsymbol{\theta}}_{1}} = -\frac{1}{n} \cdot \boldsymbol{Z}' (\boldsymbol{X} \hat{\boldsymbol{s}}_{1}' + \hat{\boldsymbol{s}}_{1} \boldsymbol{X}') \boldsymbol{Z}.
\end{align*}


\subsection{Specification testing} \label{sec:specTesting}
\citet{AreBon1991} suggest a test for second order serial correlation in the idiosyncratic remainder components. The test is generalized to higher orders $j$ by \citet{Are2003} and can be used as a specification test in the estimation of linear dynamic panel data models. The reasoning is that, although, first order serial correlation is present in the idiosyncratic remainder components for GMM estimation based on first-differenced equations, no higher-order autocorrelation beyond the first-order should prevail. The serial correlation test of \citet{AreBon1991} boils down to checking if the deviation of the covariance of the residuals of period $t$ with the residuals of period $t-j$ from zero is large enough to indicate that $j$-th order serial correlation might be present in the idiosyncratic remainder components. The null hypothesis of the test is that there is no serial correlation in the $\varepsilon_{i,t}$. The corresponding test statistics are defined as
\begin{align*}
T_{m_j} = \frac{\hat{r}_j}{\hat{\sigma}_{\hat{r}_j}}, \qquad \text{with} \qquad T_{m_j} \overset{a}{\sim} \mathcal{N}(0,1),
\end{align*}
where $\hat{\sigma}_{\hat{r}_j}$ is the standard error of the $j$-th order autocovariance of the residuals $\hat{r}_j$. For the linear dynamic panel data model specified in Equation (\ref{EQ01:lin-dyn-pdm}), this autocovariance of the residuals is the sample equivalent to
\begin{align*}
r_j = \frac{1}{T-3-j} \cdot \sum_{t=4+j}^{T} r_{t,j}, \qquad \text{with} \qquad r_{t,j} = E(\Delta s_{i,t} \Delta s_{i,t-j}),
\end{align*}
the average $j$-th order autocovariance of the idiosyncratic remainder components \citep[see][]{Are2003}. As detailed by \citet{AreBon1991} and \citet{DooAreBon2012dpd}, the corresponding scaled autocovariance of the residuals can be calculated by
\begin{align*}
\hat{r}_{t,j} = \frac{1}{\sqrt{n}} \cdot \hat{\boldsymbol{s}}_{t}' \ \hat{\boldsymbol{s}}_{t-j},
\end{align*}
where $\hat{\boldsymbol{s}}_{t}$ and $\hat{\boldsymbol{s}}_{t-j}$ are column vectors which may contain the residuals from one-step, two-step, or iterated GMM estimation for all cross sectional units at the respective time period; the index at $\hat{\boldsymbol{s}}_{t-j}$ indicates that the corresponding residuals are lagged $j$ time periods. According to \citet{AreBon1991}, the estimated variance of the $j$-th order autocovariance of the residuals is available from
\begin{align*}
\hat{\sigma}_{\hat{r}_j}^2 = \frac{1}{n} \cdot & \hat{\boldsymbol{s}}_{t-j}' \ \widehat{\boldsymbol{\Omega}}({\hat{\boldsymbol{s}}}) \hat{\boldsymbol{s}}_{t-j} - 2 \cdot \hat{\boldsymbol{s}}_{t-j}' \ \boldsymbol{X}(\boldsymbol{X}' \boldsymbol{Z} \widehat{\boldsymbol{W}} \boldsymbol{Z}' \boldsymbol{X})^{-1} \boldsymbol{X}' \boldsymbol{Z} \widehat{\boldsymbol{W}} \boldsymbol{Z}' \widehat{\boldsymbol{\Omega}}({\hat{\boldsymbol{s}}}) \hat{\boldsymbol{s}}_{t-j} \ + \\
& \hat{\boldsymbol{s}}_{t-j}' \ \boldsymbol{X} \widehat{\boldsymbol{\Omega}}(\hat{\boldsymbol{\theta}}) \boldsymbol{X}' \hat{\boldsymbol{s}}_{t-j}.
\end{align*}
Note that the vectors of residuals $\hat{\boldsymbol{s}}_t$, $\hat{\boldsymbol{s}}_{t-j}$ and the matrices $\widehat{\boldsymbol{W}}$, $\widehat{\boldsymbol{\Omega}}({\hat{\boldsymbol{s}}})$, and $\widehat{\boldsymbol{\Omega}}(\hat{\boldsymbol{\theta}})$ depend on the actual estimation step and the latter two matrices also depend on the estimated type of variance covariance matrix (i.e., robust or asymptotic for one-step estimation; Windmeijer-corrected or asymptotic for two-step estimation).


\subsection{Overidentifying restrictions testing} \label{sec:oirTesting}
When the system of equations from which the model parameters are estimated by GMM is overidentified (i.e., when the number of moment conditions exceeds the number of parameters to be estimated), it is possible to assess the validity of the overidentifying restrictions by the Sargan test \citep{Sar1958estimation}. The presumed null hypothesis is that the overidentifying restrictions are valid.
According to \citet{AreBon1991} and \citet{DooAreBon2012dpd}, the test statistic of the Sargan test can be computed from
\begin{align*}
T_S = n \cdot \hat{\boldsymbol{s}}_{1}' \boldsymbol{Z} \widehat{\boldsymbol{W}}_1 \boldsymbol{Z}' \hat{\boldsymbol{s}}_{1} \cdot \hat{\sigma}_{1}^{-2}, \quad \text{with} \quad T_S \overset{a}{\sim} \chi^2(m - p).
\end{align*}
Under suitable conditions, which ensure asymptotic normality of the GMM estimator and the additional assumption of conditional homoscedasticity, the test statistic is asymptotically $\chi^2$-distributed with $m-p$ degrees of freedom; $m$ equals the number of moment conditions employed in estimation \citep[see, e.g.,][]{Hay2000econometrics}.

An alternative test statistic, where a finite fourth moments assumption is imposed instead of conditional homoscedasticity, is the $J$-test \citep{Han1982large}. The $J$-test statistic results from replacing $\widehat{\boldsymbol{W}}_{1}$ in the above formula by $\widehat{\boldsymbol{W}}_{2}$, the one-step residuals by the two-step residuals,
\begin{align*}
T_J = n \cdot \hat{\boldsymbol{s}}_{2}' \boldsymbol{Z} \widehat{\boldsymbol{W}}_{2} \boldsymbol{Z}' \hat{\boldsymbol{s}}_{2}, \quad \text{with} \quad T_J \overset{a}{\sim} \chi^2(m - p).
\end{align*}
The idea underlying the test statistics $T_S$ and $T_J$ is, that when the moment conditions are valid, the sample analogues of these conditions should be close to zero. A large value of the test statistic indicates that some of the moment conditions may be invalid, that some of the model assumptions may be incorrect, or both \citep[see, e.g.,][]{Hay2000econometrics}.

These two tests also allow us to check the validity of nested subsets of moment conditions. These tests are referred to as `difference-in-Hansen'/`difference-in-Sargan' tests \citep[see, e.g.,][]{Roo2009note}, `incremental Hansen'/`incremental Sargan' tests \citep[see, e.g.,][]{Are2003}, or $C$-statistics \citep[see, e.g.,][]{Hay2000econometrics}. The test statistic is obtained by first estimating the null model with a restricted set of moment conditions and the full model with more moment conditions than in the null model. Then, one computes either $T_S$ or $T_J$  for both models, and then take the difference of these two test statistics. This difference is asymptotically $\chi^2$-distributed with degrees of freedom equal to the difference in the number of moment conditions under the full model and the null model \citep[see][]{Hay2000econometrics}.


\subsection{Testing linear hypotheses} \label{sec:linHyp}
The Wald test is one possibility to test general linear hypotheses of the form $H_0: \boldsymbol{R} \boldsymbol{\theta} = \boldsymbol{r}$, where the matrix $\boldsymbol{R}$ is a $c \times p$ matrix, which selects the elements of the $p \times 1$ vector of population parameters $\boldsymbol{\theta}$ required to express the left-hand side of the $c$ equations of the null hypothesis (i.e., the restrictions under the null) and the vector $\boldsymbol{r}$ is a $c \times 1$ vector that states the right-hand side of the equations. The Wald statistic can be obtained from
\begin{align*}
T_W = n \cdot (\boldsymbol{R} \hat{\boldsymbol{\theta}} - \boldsymbol{r})' \left( \boldsymbol{R} \ \widehat{\boldsymbol{\Omega}}(\hat{\boldsymbol{\theta}}) \ \boldsymbol{R}' \right)^{-1} (\boldsymbol{R} \hat{\boldsymbol{\theta}} - \boldsymbol{r}), \quad \text{with} \quad T_W \overset{a}{\sim} \chi^2(c).
\end{align*}
Note that the covariance matrix $\widehat{\boldsymbol{\Omega}}(\hat{\boldsymbol{\theta}})$ and the estimator $\boldsymbol{\theta}$ depend on the actual estimation step and the former depends on the type of covariance matrix (i.e., robust or asymptotic for one-step estimation; Windmeijer-corrected or asymptotic for two-step estimation). As usual, a large value of the Wald statistic casts doubt on the null hypothesis.

Tests of three different standard null hypotheses are currently available in \pkg{pdynmc}: (a) all coefficients corresponding to the lagged-dependent and covariates are zero jointly, (b) all coefficients corresponding to the time dummies are zero jointly, or both (a) and (b).











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section[Sample session]{Sample session} \label{sec:example}
The functionality of \pkg{pdynmc} is illustrated by replicating some of the empirical results in \citet{AreBon1991}. Additionally, we show how to incorporate the linear AB and the nonlinear AS moment conditions into the analysis. We explain all arguments which need to be set to reproduce the results and point out some alternative options. We also draw comparisons between \pkg{pdynmc}, the \proglang{Stata} implementations \pkg{xtabond2} \citep{Roo2018xtabond2}, \pkg{xtdpdgmm} \citep{Kri2019}, and the \pkg{pgmm} \citep{CroEtAl2017plm} function in the \proglang{R}-package \pkg{plm} -- where we are aware of differences between the implementations.

The data set employed in \citet{AreBon1991} is an unbalanced panel of $n=140$ firms located in the UK observed over $T=9$ time periods and is available from \proglang{R} package \pkg{plm}:
\begin{verbatim}
data(EmplUK, package = "plm")
dat                  <- EmplUK
dat[,c(4:7)]         <- log(dat[,c(4:7)])
names(dat[,c(4:7)])  <- c("n", "w", "k", "ys")
\end{verbatim}

The authors investigate employment equations and consider the dynamic specification
\begin{align} \label{EQ18:ABEstimation}
n_{i,t} =& \alpha_1 n_{i,t-1} + \alpha_2 n_{i,t-2} + \\
&\beta_1 w_{i,t} + \beta_2 w_{i,t-1} + \beta_3 k_{i,t} + \beta_4 k_{i,t-1} + \beta_5 k_{i,t-2} + \beta_6 ys_{i,t} + \beta_7 ys_{i,t-1} + \beta_8 ys_{i,t-2} + \nonumber \\
& \gamma_3 d_3 + \dots + \gamma_T d_T + \eta_i + \varepsilon_{i,t}, \qquad i = 1,...,n;\ t = 3,...,T. \nonumber
\end{align}
In the equation, $i$ denotes the firm and $t$ is the time series dimension. The natural logarithm of employment $n$ is explained by its first two lags and the further explanatory variables natural logarithm of wage $w$, natural logarithm of capital $k$, natural logarithm of output $ys$, and their lags of order up to one (for $w$) or two (for $k$ and $ys$). The variables $d_3, \dots, d_T$ are time dummies with corresponding coefficients $\gamma_3, \dots, \gamma_T$; the unobserved individual-specific effect is represented by $\eta$, and $\varepsilon$ is an idiosyncratic remainder component. The goal is to estimate the lag parameters $\alpha_1$ and $\alpha_2$ and the coefficients of the further explanatory variables $\beta_j$, with $j=1, \dots, 8$, while controlling for (unobserved) time effects and accounting for unobserved individual-specific heterogeneity.










\subsection{GMM estimation with HNR moment conditions} \label{sec:HNRmcEstimation}
When reproducing the results in Table 4 on p.290 of \citet{AreBon1991} with \pkg{pdynmc}, the model structure underlying Equation (\ref{EQ18:ABEstimation}) can be specified by:
\begin{verbatim}
m1 <- pdynmc(
  dat = dat, varname.i = "firm", varname.t = "year",
  use.mc.diff = TRUE, use.mc.lev = FALSE, use.mc.nonlin = FALSE,
  include.y = TRUE, varname.y = "emp", lagTerms.y = 2,
  fur.con = TRUE, fur.con.diff = TRUE, fur.con.lev = FALSE,
  varname.reg.fur = c("wage", "capital", "output"),
  lagTerms.reg.fur = c(1,2,2),
  include.dum = TRUE, dum.diff = TRUE, dum.lev = FALSE, varname.dum = "year",
  w.mat = "iid.err", std.err = "corrected",
  estimation = "onestep", opt.meth = "none")
\end{verbatim}
The first arguments relate to the data set (\code{dat}), the cross-section identifier (\code{varname.i}), and time series dimension (\code{varname.t}).
Next, the moment conditions are defined -- here, only moment conditions from equations in differences are used.
The moment conditions are derived for the dependent variable (\code{varname.y}), and for the corresponding number of lags of the dependent variable to be included as explanatory variables (\code{lagTerms.y}).
Further explanatory variables aside from the lags of the dependent variable (\code{fur.con}) are included in the equations in differences, not in the equations in levels (compare \code{fur.con.diff = TRUE, fur.con.lev = FALSE}), their variable names are stated in (\code{varname.reg.fur}), their lag structure is specified in (\code{lagTerms.reg.fur}).
Note that the first element of the vector denoting the lag structure corresponds to the first element of the vector with the variable names, the second element to the second, and so on.
Also note that all names given in the vectors that refer to variables in the data set need to have the same names as in the data set.
The time dummies are included by (\code{include.dum}) into the equations in differences not in the equations in levels (\code{dum.diff = TRUE, dum.lev = FALSE}), the dummy indicator variable is (\code{varname.dum}). Note that time dummies can be constructed from one or multiple variables by \pkg{pdynmc} by simply passing a scalar or vector with the respective variable names in the data to \code{varname.dum}.
Specifying the matrix $\boldsymbol{H}$ in Equation (\ref{EQ10:W1step}), which governs the structure of the one-step weighting matrix, and carrying out one-step estimation can be achieved by the commands in the last two codelines. Choosing the option \code{iid.err} uses the matrix $\boldsymbol{H}_{HNR}$ proposed by \citet{AreBon1991}. Alternatively, an identity matrix can be employed for $\boldsymbol{H}$ by the option \code{identity}.
Code \code{std.err = "corrected"} yields robust standard errors for one-step estimation (in case of a two-step estimation, the correction of \citealp{Win2005} was employed by this argument).

One-step, two-step, and iterated GMM are carried out by numerical optimization of the GMM objective function given in Equation (\ref{EQ09:Objective-function-GMM}). Since a closed form solution exists for the estimator when employing only linear moment conditions, numerical optimization is not required and can be switched off by setting \code{opt.meth = "none"}.

The standard output can be accessed via \code{summary(m1)} and could be found in panel (a) of Table \ref{Tab01:estimates}. These reproduce the coefficient estimates in Table 4, column (a1) on p.290 of \citet{AreBon1991}, when one specifies all arguments as stated in this section.

Changing the argument \code{estimation} to \code{twostep} yields the two-step GMM coefficient estimates (the \code{pdynmc}-output object is assigned to \code{m2}) from Table 4, column (a2) on p.290 of \citet{AreBon1991}. These results may be found in panel (b) of Table \ref{Tab01:estimates}.

Note that the standard errors presented in column (b) of Table \ref{Tab01:estimates} are based on the Windmeijer-correction and deviate from the conventional standard errors reported in \citet{AreBon1991}. The standard errors from the original analysis can be reproduced by setting \code{std.err = "unadjusted"}.

\begin{landscape}
\begin{table}[hptb]
\centering
\begin{scriptsize}
\caption{Estimates in the spirit of Table 4 in \citet{AreBon1991}} \label{Tab01:estimates}
\begin{tabular}{lcccccccccc}
  \hline
 & \multicolumn{2}{c}{{(a)}} & \multicolumn{2}{c}{{(b)}} & \multicolumn{2}{c}{{(c)}} &
 \multicolumn{2}{c}{{(d)}}  & \multicolumn{2}{c}{{(e)}}\tabularnewline
 & \multicolumn{1}{c}{{Estimate}} & \multicolumn{1}{c}{{Std.Err.rob}}  & \multicolumn{1}{c}{{Estimate}} & \multicolumn{1}{c}{{Std.Err.rob}} & \multicolumn{1}{c}{{Estimate}} & \multicolumn{1}{c}{{Std.Err.rob}} & \multicolumn{1}{c}{{Estimate}} & \multicolumn{1}{c}{{Std.Err.rob}} & \multicolumn{1}{c}{{Estimate}} & \multicolumn{1}{c}{{Std.Err.rob}}\tabularnewline
  \hline
  L1.n  & 0.68623{***}  & 0.14459   & 0.62871{**}   & 0.19341   & 1.10335{***}  & 0.05002   & 1.08852{***}  & 0.07476   & 1.16847{***}  & 0.06239 \tabularnewline
  L2.n  & -0.08536      & 0.05602   & -0.06519      & 0.04505   & -0.10423{*}   & 0.04652   & -0.05953      & 0.07776   & -0.19362{***} & 0.05663 \tabularnewline
  w     & -0.60782{***} & 0.17821   & -0.52576{***} & 0.15461   & -0.44828{**}  & 0.14872   & -0.53856{***} & 0.11410   & -0.21777      & 0.12304 \tabularnewline
  L1.w  & 0.39262{*}    & 0.16799   & 0.31129       & 0.20300   & 0.42262{**}   & 0.15564   & 0.52214{***}  & 0.11837   & 0.17753       & 0.12743 \tabularnewline
  k     & 0.35685{***}  & 0.05902   & 0.27836{***}  & 0.07280   & 0.29027{***}  & 0.04999   & 0.34252{***}  & 0.05727   & 0.22181{***}  & 0.04912 \tabularnewline
  L1.k  & -0.05800      & 0.07318   & 0.01410       & 0.09246   & -0.15286{*}   & 0.06704   & -0.21398{**}  & 0.06679   & -0.07124      & 0.06904 \tabularnewline
  L2.k  & -0.01995      & 0.03271   & -0.04025      & 0.04327   & -0.13719{***} & 0.04127   & -0.15273{***} & 0.03866   & -0.12912{**}  & 0.04568 \tabularnewline
  ys    & 0.60851{***}  & 0.17253   & 0.59192{***}  & 0.17309   & 0.54843{**}   & 0.19379   & 0.59404{**}   & 0.18922   & 0.29825       & 0.17402 \tabularnewline
  L1.ys & -0.71116{**}  & 0.23172   & -0.56599{*}   & 0.26110   & -0.66598{**}  & 0.22142   & -0.77509{***} & 0.22750   & -0.39531      & 0.21439 \tabularnewline
  L2.ys & 0.10580       & 0.14120   & 0.10054       & 0.16110   & 0.12729       & 0.15561   & 0.17620       & 0.15027   & 0.12557       & 0.13491 \tabularnewline
  1979  & 0.00955       & 0.01029   & 0.01122       & 0.01168   & 0.02354{*}    & 0.01097   & 0.02278{***}  & 0.00621   & 0.01567       & 0.00969 \tabularnewline
  1980  & 0.02202       & 0.01771   & 0.02307       & 0.02006   & 0.04106{*}    & 0.01953   & 0.04178{*}    & 0.01800   & 0.02034       & 0.01929 \tabularnewline
  1981  & -0.01177      & 0.02951   & -0.02136      & 0.03324   & 0.00215       & 0.03389   & 0.01271       & 0.02742   & -0.04586      & 0.03014 \tabularnewline
  1982  & -0.02706      & 0.02928   & -0.03112      & 0.03397   & 0.01762       & 0.02289   & 0.01554       & 0.02401   & 0.00703       & 0.02050 \tabularnewline
  1983  & -0.02132      & 0.03046   & -0.01799      & 0.03693   & 0.04332{*}    & 0.01803   & 0.03006       & 0.01730   & 0.04808{*}    & 0.01886 \tabularnewline
  1984  & -0.00770      & 0.03141   & -0.02337      & 0.03661   & 0.02912       & 0.02161   & 0.02610       & 0.03956   & 0.02236       & 0.02069 \tabularnewline
   \hline
   \hline
    \multicolumn{11}{l}{{\scriptsize{}(a) one-step estimates; instruments for equations in first differences: $L\left(2/8\right).n,D.w,L.D.w,D.k,L.D.k,L2.D.k,D.ys,L.D.ys,L2.D.ys,D.1979-D.1984$}}\tabularnewline
    \multicolumn{11}{l}{{\scriptsize{}(b) two-step estimates; instruments for equations in first differences: $L\left(2/8\right).n,D.w,L.D.w,D.k,L.D.k,L2.D.k,D.ys,L.D.ys,L2.D.ys,D.1979-D.1984$}}\tabularnewline
    \multicolumn{11}{l}{{\scriptsize{}(c) two-step estimates; equations in first differences: $L\left(2/8\right).n,D.w,L.D.w,L2.D.w,D.k,L.D.k,L2.D.k,D.ys,L.D.ys,L2.D.ys,D.1979-D.1984$}}\tabularnewline
    \multicolumn{11}{l}{{\scriptsize{}\phantom{(c) two-step estimates; }equations in levels: $L\left(1/7\right).D.n,w,L.w,L2.w,k,L.k,L2.k,ys,L.ys,L2.ys$}}\tabularnewline
    \multicolumn{11}{l}{{\scriptsize{}(d) two-step estimates; equations in first differences: $L\left(2/8\right).n, u ,D.w,L.D.w,L2.D.w,D.k,L.D.k,L2.D.k,D.ys,L.D.ys,L2.D.ys,D.1979-D.1984$}}\tabularnewline
    \multicolumn{11}{l}{{\scriptsize{}\phantom{(d) two-step estimates; }equations in levels: $w,L.w,L2.w,k,L.k,L2.k,ys,L.ys,L2.ys$}}\tabularnewline
    \multicolumn{11}{l}{{\scriptsize{}(e) 17 step estimates; equations in first differences: $L\left(2/8\right).n,D.w,L.D.w,L2.D.w,D.k,L.D.k,L2.D.k,D.ys,L.D.ys,L2.D.ys,D.1979-D.1984$}}\tabularnewline
    \multicolumn{11}{l}{{\scriptsize{}\phantom{(e) 17 step estimates; }equations in levels: $L\left(1/7\right).D.n,w,L.w,L2.w,k,L.k,L2.k,ys,L.ys,L2.ys$}}\tabularnewline
    \multicolumn{11}{l}{{\scriptsize{}{*} $p<0.05$, {**} $p<0.01$, {***} $p<0.001$ (refers to $t$-test of the null that the coefficient is equal to zero)}}\tabularnewline
\end{tabular}
\end{scriptsize}
\end{table}
\end{landscape}

Regarding the arguments
\begin{verbatim}
  use.mc.diff = TRUE, include.y = TRUE, include.x = FALSE
\end{verbatim}
it has to be noted that the HNR moment conditions (\code{use.mc.diff}) derived from the lagged dependent variable (\code{include.y}) are employed, while none are derived from other explanatory variables (by default \code{include.x = FALSE}). The latter argument implies that these other explanatory variables in the model are assumed to be exogenous and instrument themselves.

Different capabilities for testing hypotheses about the population parameters are available in \pkg{pdynmc}. Among them are the tests for serial correlation in the idiosyncratic remainder components proposed by \citet{AreBon1991},
%Sargan tests,
Hansen tests, and Wald tests. In the following, carrying out these tests and interpreting the results is briefly illustrated based on the two-step GMM estimation results presented in column (b) of Table \ref{Tab01:estimates}.

Employing the test for second order serial correlation of \citet{AreBon1991} described in Section \ref{sec:specTesting} by \code{m.test(m2, t.order = 2)} yields:
\begin{verbatim}
        Serial correlation test of degree 2

data:  GMM Estimation; H0: no serial correlation of order 2 in epsilon
normal = -0.36744, p-value = 0.7133
\end{verbatim}
The test does not reject the null hypothesis at any plausible significance level and does not provide any indication that the model specification might be inadequate. The test statistic and $p$-value are similar to \pkg{xtabond2} and \pkg{pgmm}.

Computing the Hansen $J$-test of the overidentifying restrictions described in Section \ref{sec:oirTesting} by \code{j.test(m2)} yields:
\begin{verbatim}
        J-Test of Hansen

data:  GMM Estimation; H0: overidentifying restrictions valid
chisq = 31.381, df = 25, p-value = 0.1767
\end{verbatim}
The test does not reject the overidentifying restrictions and does not provide any indications that the validity of the instruments employed in estimation may be in doubt. Comparing the results to \pkg{xtabond2} shows that the degrees of freedom and the $p$-value differ. We consider 25 degrees of freedom to be the appropriate number here, as 41 instruments are employed in estimation to obtain 16 coefficient estimates. It seems that the function \pkg{xtabond2} does not correct the degrees of freedom for the number of dummies dropped in estimation\footnote{Dummies are dropped by the estimation routine in case of high collinearity.}. The difference in the $p$-value is due to the differences in the degrees of freedom. Our results are equivalent to the results of \pkg{pgmm} for the overidentifying restrictions test. In \pkg{pgmm}, the above test is referred to as `Sargan test'.

For the Wald test illustrated in Section \ref{sec:linHyp}, consider the null hypothesis that the population parameters of all coefficients included in the model are zero jointly, which is tested by \code{wald.fct(param = "all", object = m2}:
\begin{verbatim}
        Wald test

data:  GMM Estimation; H0: beta = 0; tested model parameters: all
chisq = 1104.7, df = 16, p-value < 2.2e-16
\end{verbatim}
The test rejects the null hypothesis. Comparing the test result to the implementation of the test in \pkg{xtabond2} -- again -- reveals differences concerning the degrees of freedom. We consider 16 to be the appropriate number of degrees of freedom here, since this corresponds to the number of estimated parameters. As noted previously, the differences seem to stem from \pkg{xtabond2} not adjusting the degrees of freedom for the dummies dropped in estimation. Alternative hypotheses that can be tested via the Wald test in \pkg{pdynmc} are that all slope parameters are zero jointly and that all parameters corresponding to the time dummies are zero jointly (\code{param = "time.dum"} only tests the time dummies, while \code{param = "slope"} only tests the slope parameters).












\subsection{GMM estimation with HNR and ABov moment conditions} \label{sec:HNRABmcEstimation}
When the `constant correlated effects' assumption stated in Equation (\ref{EQ07:CCE}) holds, the HNR moment conditions from equations in differences employed in Section \ref{sec:HNRmcEstimation} can be extended by the ABov moment conditions from equations in levels.

The ABov moment conditions are particularly useful for data generating processes, which are highly persistent \citep{BluBon1998}. In this case, identification by the HNR moment conditions from equations in levels may fail and GMM estimation based on HNR moment conditions is documented to possess poor finite sample performance \citep[see, e.g.,][]{BluBon1998,BluBonWin2001,BunSar2015}.

In \pkg{pdynmc}, the ABov moment conditions from equations in levels can be (additionally) incorporated by :
\begin{verbatim}
use.mc.lev = TRUE
\end{verbatim}
In principle, both, the time dummies and the further explanatory variables can be included in the equations in first differences and the level equations. It is recommended, though, to include the dummies only in one of the equations, as it can be shown that incorporating them in both equations renders one set of dummies redundant for estimation -- while for the non-lagged dependent explanatory variables, this equivalence does not hold.\footnote{Note that this is the case in balanced panels. The results may also not be numerically identical across function calls for different choices of the one-step weighting matrix. For a discussion, see https://www.statalist.org/forums/forum/general-stata-discussion/general/1357268-system-gmm-time-dummies.} We accommodate non-lagged dependent explanatory variables in the levels equations by
\begin{verbatim}
fur.con.lev = TRUE
\end{verbatim}
and use this argument together with the earlier specified ones.


In order to obtain coefficient estimates, a decision about the matrix $\boldsymbol{H}$ in the one-step weighting matrix is required. When using the HNR and ABov moment conditions, the decision about $\boldsymbol{H}$ effectively involves specifying the matrices $\boldsymbol{A}$, $\boldsymbol{B}$, and $\boldsymbol{C}$ in the general structure given in Section \ref{section:1_2_step}. As mentioned, the diagonal elements $\boldsymbol{A}$ and $\boldsymbol{C}$ reflect the expected variance covariance properties within a set of moment conditions, while $\boldsymbol{B}$ reflects the expected covariances across different sets of moment conditions. In the given setting, $\boldsymbol{A}$ corresponds to the variance covariance properties of the HNR moment conditions, $\boldsymbol{C}$ to those of the ABov moment conditions, and $\boldsymbol{B}$ to those across the HNR and ABov moment conditions. Three different options are currently available in \pkg{pdynmc} to set up the weighting matrix \code{w.mat}: \code{iid.err}, \code{identity}, and \code{zero.cov}. The first option leads to $\boldsymbol{H}_{HNR}$ being used for $\boldsymbol{A}$, an identity for $\boldsymbol{C}$, and a matrix $\boldsymbol{B}$, such that $\boldsymbol{B} \boldsymbol{B}' = \boldsymbol{H}_{HNR}$. Setting \code{w.mat} to \code{identity} leads to an identity matrix being used for the diagonal matrices $\boldsymbol{A}$ and $\boldsymbol{C}$ and an adequately dimensioned matrix $\boldsymbol{B}$ with 1 on the diagonal\footnote{Note that the matrix $\boldsymbol{B}$ is not necessarily a quadratic matrix.}.
When using the option \code{zero.cov}, the matrices $\boldsymbol{A}$ and $\boldsymbol{C}$ are as for option \code{iid.err}, but $\boldsymbol{B}$ is set to a null matrix. In case nonlinear moment conditions are used, the part of $\boldsymbol{H}$ which corresponds to the nonlinear moment conditions is set to an identity for all choices of \code{w.mat}. All elements of the matrices containing the expected covariance properties of the nonlinear moment conditions with other moment conditions are always set to zero.

The results presented in column (c) of Table \ref{Tab01:estimates} are the two-step estimates of column (a2) of Table 4 in \citet{AreBon1991} extended by the ABov moment conditions. All arguments are specified as described above. Including the ABov moment conditions into the analysis leads to substantial changes in the coefficient estimates of the first lag of the dependent variable. Note that the results indicate a markedly higher persistence of employment and render including two lags of the dependent variable questionable \citep[][e.g., estimate a version of the equation which contains only one lag of all explanatory variables]{BluBon1998}. Note that the coefficient estimates of the explanatory variables, besides the first lag of the dependent variable, appear to be similar across estimations.


Equivalent results to column (c) of Table \ref{Tab01:estimates} can be obtained from the \pkg{pgmm} function in the \pkg{plm}-package -- besides some minor numerical differences at the fifth digit. When replicating the results with \pkg{xtabond2}, differences in the implementations become obvious: The instrument set for the ABov moment conditions is extended in similar fashion to the HNR moment conditions in \pkg{xtabond2}, while this is not the case in \pkg{pgmm}. An argument is available in \pkg{pdynmc} to extend the instrument set as in \pkg{xtabond2}:
\begin{verbatim}
inst.stata = TRUE
\end{verbatim}
Due to the reasons described in Section \ref{sec:ExtAssumpt}, this argument is set to \code{FALSE} per default. When setting the option to \code{TRUE}, the results from \pkg{xtabond2} and \pkg{pdynmc} are very close to our results.








\subsection{GMM estimation with HNR and AS moment conditions}
Recall that the linear ABov moment conditions from equations in levels comprise the nonlinear AS moment conditions and render them redundant for estimation (\citealp[][]{BluBon1998}; \citealp[a derivation is provided in][]{Fri2019}). Both sets of moment conditions may be useful in GMM estimation when the lag parameter is close to unity and it can be shown that extending the HNR moment conditions by either the ABov- or the AS moment conditions may identify the lag parameter -- even when the individual moment conditions fail to do so \citep{BunKle2014,GorHanXue2016}. The ABov moment conditions require the `constant correlated effects' assumption, while the AS moment conditions only require standard assumptions to hold. Therefore, the latter may be useful in situations where the `constant correlated effects' assumption is in doubt and the statistician aims to investigate a highly persistent dynamic process with a structure similar to Equation (\ref{EQ01:lin-dyn-pdm}).
In \pkg{pdynmc}, including nonlinear moment conditions into the analysis is available via:
\begin{verbatim}
use.mc.nonlin = TRUE
\end{verbatim}

When extending the analysis of \citet{AreBon1991} by the nonlinear AS moment conditions, the results differ substantially from column (b) of Table \ref{Tab01:estimates} and are very similar to the coefficient estimates shown in column (c) of Table \ref{Tab01:estimates}. This casts doubt on the HNR moment conditions and may be a hint that there is high persistence in the employment process --  as high persistence leads to the lag parameters not being identified by the HNR moment conditions \citep{BunKle2014,GorHanXue2016}.
Note that since the unobservable error term $u$ (which can be expressed in terms of observable model components and parameters) is included in the instrument set for the equations in first differences, nonlinear moment conditions are employed in estimation. Also note that the coefficient estimates in column (d) of Table \ref{Tab01:estimates} are very close to coefficient estimates obtained from \pkg{xtdpdgmm}.






\subsection{Iterated GMM}
Iterated GMM can be used, by specifying the following commands:
\begin{verbatim}
estimation = "iterative", max.iter = 100, iter.tol = 0.01,
\end{verbatim}
When \code{estimation = "iterative"} is used, \code{max.iter} specifies the maximum number of iterations, \code{iter.tol} the search tolerance w.r.t.\ convergence. We employ \code{max.iter = 100} and \code{iter.tol = 0.01}. These values are the default values and can be adjusted by the user. Iterated GMM results are shown in column (e) of Table \ref{Tab01:estimates}. The moment conditions employed are the same as in column (c) of the table. The parameter estimates obtained after 17 steps are relatively similar to those in columns (c)-(d).



\subsection{Starting values}
If numerical optimization techniques are used, the starting for all parameters are drawn from the uniform distribution on an interval [-1, 1] by the following commands (set as default):
\begin{verbatim}
start.val.lo = -1, start.val.up = 1, seed.input = 42
\end{verbatim}
As usual, the \code{seed.input} ensures reproducibility.
The starting values can be varied by the user via arguments \code{start.val.lo} and \code{start.val.up} and setting \code{custom.start.val} to TRUE.

















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Concluding remarks}
\label{sec:conclusion}

The \proglang{R}-package \pkg{pdynmc} provides a function to estimate linear dynamic panel data models. The implementation allows for general lag structures of the explanatory variables, which may encompass lags of the dependent variable and further explanatory variables. For estimation, linear and nonlinear moment conditions are derived from the model assumptions; further controls and external instruments (if available) may also be added. Estimation is carried out by numerical optimization of the GMM objective function. Corresponding closed form solutions are computed -- where possible -- and stored besides the results from numerical optimization. The estimation routine can handle balanced and unbalanced panel data sets and provides one-step-, two-step-, and continuously updating estimation. Accounting for (unobserved) time-specific effects is possible by including time dummies.
Different choices for the weighting matrix, which guides the aggregation of moment conditions in one-step GMM estimation are available. Concerning the computation of standard errors for the coefficient estimates, the following options are currently available in \pkg{pdynmc}: non-robust one- and two-step standard errors and robust one-step- and Windmeijer-corrected two-step standard errors. Some standard hypothesis and specification tests are also available. Among them are Wald tests, overidentifying restrictions tests and a test for serial correlation in the idiosyncratic remainder components.

We plan to extend the package by the following features in the future:
{\sffamily
\begin{itemize}
    \item As mentioned in Section 2, speed improvements are a priority to enable the use of this package in Monte Carlo simulations. In addition, once the "bleeding-edge" features of iterated GMM become mainstream, having a package that provides some basic functionality will be of use.
    \item Incorporate further diagnostics and tests to assess the validity of the estimated specifications and the underlying moment conditions and assumptions (e.g., testing the `constant correlated effects' assumption and testing for structural breaks).
    \item Facilitate choosing an adequate dynamic specification by lag selection techniques.
    \item Include moment selection capabilities based on an appropriate criterion into GMM estimation which allow to remove certain instruments/moment conditions.
    \item Expand the possible choices for the one-step weighting matrix by, e.g., the proposition in \citet{Kiv2007WP} for GMM estimation based on linear HNR- and ABov moment conditions.
    \item Implement the IV estimator solely based on the nonlinear moment conditions proposed by \citet{PuaFriSch2019a,PuaFriSch2019b}.
\end{itemize}
}








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Acknowledgements}
Andrew Adrian Pua gratefully acknowledges support from the Fundamental Research Funds for the Central Universities (No. 20720171074).







\clearpage
\bibliography{REFERENCES}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\section*{Appendix}



\end{document}
