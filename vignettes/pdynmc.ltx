%\VignetteIndexEntry{pdynmc: An R-package for estimating linear dynamic panel data models based on nonlinear moment conditions}
%\VignetteEngine{R.rsp::tex}
%\VignetteKeyword{pdynmc}
%\VignetteKeyword{dynamic panel}
%\VignetteKeyword{nonlinear moment conditions}


%\documentclass[article]{jss}
\documentclass[nojss]{jss}

\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{mathrsfs}           %for neighborhood symbol
\usepackage{units}
\setcounter{MaxMatrixCols}{30}
\usepackage{arydshln}           %JS: for dashed lines





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% almost as usual
\author{Markus Fritsch\\University of Passau \And
        Andrew Adrian Yu Pua\\Xiamen University \And
        Joachim Schnurbus\\University of Passau}
\title{\pkg{pdynmc} -- An \proglang{R}-package for estimating linear dynamic panel data models based on nonlinear moment conditions}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Markus Fritsch, Andrew Adrian Yu Pua, Joachim Schnurbus} %% comma-separated
\Plaintitle{pdynmc -- An R-package for estimating linear dynamic panel data models based on nonlinear moment conditions} %% without formatting
\Shorttitle{\pkg{pdynmc} -- An R-package employing nonlinear moment conditions ...} %% a short title (if necessary)

%% an abstract and keywords
\Abstract{
\pkg{pdynmc} is an \proglang{R}-package for IV- or GMM-estimation of linear dynamic panel data models that are based on nonlinear moment conditions as proposed by \citet{AhnSch1995}.
This paper provides a description of the variety of options regarding instrument type, covariate type, estimation methodology, and general configuration from the perspective of an applied statistician.
All functionality is demonstrated for a publicly available unbalanced panel data set \citep{AreBon1991} and we relate to other software and packages.
}
\Keywords{dynamic panel, generalized method of moments, instrument variable, nonlinear moment conditions, \proglang{R}}
\Plainkeywords{dynamic panel, generalized method of moments, instrument variable, nonlinear moment conditions, R} %% without formatting
%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Markus Fritsch\\
  Chair of Statistics and Data Analytics\\
  School of Business, Economics, and Information Systems\\
  University of Passau\\
  94032 Passau, Germany\\
  E-mail: \email{markus.fritsch@uni-passau.de}\\
  URL: \url{https://www.wiwi.uni-passau.de/en/statistics/team/}
}
%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/512/507-7103
%% Fax: +43/512/507-2851

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{document}

%% include your article here, just as usual
%% Note that you should use the \texttt{}, \texttt{} and \texttt{} commands.

\section{Introduction}
%\section[Introduction]{Introduction}
%% Note: If there is markup in \(sub)section, then it has to be escape as above.
The linear dynamic panel data model allows to account for dynamics and unobserved individual-specific heterogeneity simultaneously. Due to the presence of unobserved individual-specific effects and lagged dependent variables, standard estimation techniques like pooled ordinary least squares (OLS) or the within estimation generally do not lead to consistent estimates \citep[see, e.g.,][]{Hsi2014}.
%\citep[see, e.g.,][p.82-86]{Hsi2014}.
A suitable alternative for obtaining parameter estimates in linear dynamic panel data models is deriving moment conditions (or population orthogonality conditions) from the model assumptions. The moment conditions may be linear \citep{AndHsi1982,HolNewRos1988,AreBov1995} or nonlinear \citep{AhnSch1995} in parameters and determine the natural instruments available for estimation. Usually, the number of moment conditions exceeds the number of parameters and the moment conditions need to be aggregated appropriately. This can be achieved by the generalized method of moments (GMM), where (weighted) linear combinations of the moment conditions are employed to obtain parameter estimates.


Theoretical results and evidence from Monte Carlo simulations in the literature suggest that incorporating the nonlinear moment conditions proposed by \citet{AhnSch1995} may prove valuable for particular data generating processes (DGPs). One example is when the process exhibits high persistence and the linear moment conditions fail to identify the model parameters: The nonlinear moment conditions may still provide identification \citep{BunKle2014,BunSar2015,GorHanXue2016}.
Further note that the nonlinear moment conditions only impose standard assumptions about the (unknown) underlying DGP. Despite these results, however, and although the nonlinear moment conditions were proposed by Ahn and Schmidt more than 20 years ago, standard estimation routines are generally not available across statistical software. To the best of our knowledge, there is currently only the implementation provided by \citep{Kri2019} for the commercial statistical software \proglang{Stata} \citep{Sta2011} that is explicitly designed to incorporate nonlinear moment conditions into GMM estimation.


Our package \pkg{pdynmc} provides an implementation of GMM estimation of linear dynamic panel data models based on different sets of moment conditions in the statistical open source software \proglang{R} \citep{RCo2019}. The building blocks from which the sets of moment conditions available for GMM estimation can be constructed are the nonlinear (in parameters) \citet{AhnSch1995} moment conditions and the two different types of linear moment conditions proposed by \citet{HolNewRos1988} and \citet{AreBov1995}. Our package allows to use various combinations of these moment conditions to obtain parameter estimates. In their standard form, the \citet{HolNewRos1988}, \citet{AreBov1995}, and \citet{AhnSch1995} moment conditions are derived from the lagged dependent variable. Additional moment conditions, which may arise from assumptions about the non-lagged dependent explanatory variables, can also be included in estimation. Since the moment conditions employed in GMM estimation of linear dynamic panel data models are derived from model assumptions, a basic understanding of these assumptions is vital for setting up a plausible estimation routine. The methodological part of this paper briefly reviews the assumptions implied when using particular moment conditions in estimation and provides further references for more detailed overviews.


The structure of the paper is as follows. Section \ref{sec:ldpdm} briefly sketches the linear dynamic panel data model, states the underlying assumptions frequently used in the literature, and describes the moment conditions arising from the model assumptions. Section \ref{sec:gmm} covers GMM estimation of linear dynamic panel data models and illustrates the minimization criterion, estimation in one, two, or multiple steps, and closed form solutions.
Section 4 outlines the computation of standard errors, specification- and overidentifying restrictions testing, and the testing of general linear hypotheses.
Related software and \proglang{R}-packages are summarized in Section \ref{sec:related software}. Section \ref{sec:example} illustrates the estimation of linear dynamic panel data models with \pkg{pdynmc} for the data set of \citet{AreBon1991} on adjustments of employment of firms located in the United Kingdom.
Section \ref{sec:conclusion} concludes and sketches functionality we plan to add to future releases of the package.
















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Linear dynamic panel data model} \label{sec:ldpdm}
\subsection{Model and standard assumptions}
For a given sample with a cross section dimension $n$ and a time series dimension $T$, consider the two equations
\begin{align}
y_{i, t} & = \alpha y_{i, t - 1} + \beta x_{i, t} + u_{i,t}, \quad i = 1, \dots, n;\ t = 2, \dots, T,\label{EQ00-1:lin-dyn-pdm} \\
u_{i,t} & = \eta_{i} + \varepsilon_{i, t}, \label{EQ00-2:lin-dyn-pdm}
\end{align}
where $y_{i,t}$ and $y_{i,t-1}$ denote the dependent variable and its lag, $\alpha$ is the lag parameter, and $x_{i,t}$ is a non-lagged dependent explanatory variable with corresponding slope coefficient $\beta$. The second equation requires that the (unobserved) composite error term $u_{i,t}$ can be separated into an unobserved individual-specific effect $\eta_i$ and an idiosyncratic remainder component $\varepsilon_{i,t}$.\footnote{We only include one lag of the dependent variable, one non-lagged dependent explanatory variable, and omit unobserved time-specific effects for simplicity of exposition and notational convenience. Extending the representation is straightforward. Unobserved time-specific effects can, e.g., be incorporated by including time dummies.} The initial time period is denoted by $t=1$.


Combining the Equations (\ref{EQ00-1:lin-dyn-pdm}) and (\ref{EQ00-2:lin-dyn-pdm}) yields the single equation form of the model
\begin{align}
y_{i, t} & = \alpha y_{i, t - 1} + \beta x_{i, t} + \eta_{i} + \varepsilon_{i, t}, \quad i = 1, \dots, n;\ t = 2, \dots, T \label{EQ01:lin-dyn-pdm}.
\end{align}


We impose the following set of standard assumptions (SA) from the literature \citep[see][]{AhnSch1995}:
\begin{align} \label{EQ02:StdAssumpt}
& \text{The data are independently distributed across} \ i, \\
& E(\eta_i) = 0, \quad i=1,...,n, \nonumber \\
& E(\varepsilon_{i,t}) = 0, \quad i=1,...,n;\ t=2,...,T, \nonumber \\
& E(\varepsilon_{i,t} \cdot \eta_i) = 0, \quad i=1,...,n;\ t=2,...,T, \nonumber \\
& E(\varepsilon_{i,t} \cdot \varepsilon_{i,s}) = 0, \quad i=1,...,n;\ t \neq s, \nonumber \\
& E(y_{i,1} \cdot \varepsilon_{i,t}) = 0, \quad i=1,...,n;\ t=2,...,T, \nonumber \\
%& \alpha \in \left[-1,1\right] \setminus{\{0\}}, \nonumber \\
& n \rightarrow \infty, \ \text{while}\ T \  \text{is fixed, such that}\ \frac{n}{T} \rightarrow 0. \nonumber
\end{align}
The six assumptions in Equation (\ref{EQ02:StdAssumpt}) imply: First, the assumption that the data are independently distributed across individuals allows for dependence of the model components across time, but not across individuals. Second, the unobserved individual-specific effect and the idiosyncratic remainder component need to be zero in expectation (if this is not the case, a constant can be included in the model to ensure the property). Third, orthogonality of the $\varepsilon_{i,t}$ with the following model components is required: The unobserved individual-specific effects, the idiosyncratic remainder components of all other time periods, and the initial conditions of the $y_{i,t}$-process.
Due to the zero mean assumption concerning the $\varepsilon_{i,t}$, uncorrelatedness follows from orthogonality of these model components. The last assumption requires that the cross section dimension is large, while the time series dimension is finite.








\subsection{Moment conditions from standard assumptions} \label{sec:StdAssumpt}
Usual approaches in applied statistics obtain (OLS) estimates of the model parameters of Equation (\ref{EQ01:lin-dyn-pdm}) by: (i) ignoring the unobserved individual-specific effects, (ii) deducting the individual-specific mean over time from all left-hand- and right-hand side variables (also referred to as the within transformation) of the equation, or (iii) including one dummy per observation in the estimation (the least squares dummy variables -- or LSDV -- approach; the within estimation and LSDV yield identical slope coefficient estimates). However, due to the presence of the lagged dependent variable and the unobserved individual-specific effects, the techniques (i)-(iii) do not yield consistent estimates without imposing additional restrictions on the model \citep[see, e.g.,][]{Hsi2014}.
%\citep[see, e.g.,][p.82-86]{Hsi2014}.


The unobserved individual-specific effects can be eliminated from Equation (\ref{EQ01:lin-dyn-pdm}) by first differencing the equation. Utilizing the $\Delta$-operator to indicate the first differencing gives
\begin{align} \label{EQ03:FDlin-dyn-pdm}
\Delta y_{i, t} = \alpha \Delta y_{i, t - 1} + \beta \Delta x_{i, t} + \Delta \varepsilon_{i, t}, \quad i = 1, \dots, n;\ t = 2, \dots, T.
\end{align}
Due to the first differenced lagged dependent variables $\Delta y_{i,t-1} = y_{i,t-1} - y_{i,t-2}$ and the first differenced error terms $\Delta \varepsilon_{i,t} = \varepsilon_{i,t} - \varepsilon_{i,t-1}$ not being orthogonal, estimating Equation (\ref{EQ03:FDlin-dyn-pdm}) with OLS still leads to biased coefficient estimates. The standard assumptions stated in Equation (\ref{EQ02:StdAssumpt}) provide a remedy: The assumptions imply two sets of moment conditions, whose sample analogues can be used in estimation. Note that the following moment conditions refer to the population and that the expectation is taken over the cross section dimension.


\citet{HolNewRos1988} (hereafter HNR) propose the linear (in parameters) moment conditions
\begin{align}\label{EQ04:HNR-MC-linear}
E(y_{i,s} \cdot \Delta u_{i,t}) = 0, \qquad t = 3,\dots,T;\ s = 1,\dots,t - 2.
\end{align}
Depending on the time series dimension available for estimation, Equation (\ref{EQ04:HNR-MC-linear}) provides $0.5(T-1)(T-2)$ moment conditions.
Equivalent moment conditions can be derived from the non-lagged dependent explanatory variables. Endogenous ($x^{\text{end}}$), predetermined ($x^{\text{pre}}$), and (strictly) exogenous ($x^{\text{ex}}$) variables provide the linear moment conditions (see the Equations (9.5)-(9.7) of \citealp{BluBonWin2001}):
\begin{align} \label{EQ06:HNR-MC-linear-x_it}
E(x_{i,s} \cdot \Delta u_{i,t}) = 0, \qquad t = 3,\dots,T, \qquad &\text{where}& \\
s = 1,\dots,t - 2, \qquad &\text{for}& x = x^{\text{end}}, \nonumber \\
s = 1,\dots,t - 1, \qquad &\text{for}& x = x^{\text{pre}}, \nonumber \\
s = 1,\dots,T,     \qquad &\text{for}& x = x^{\text{ex}}. \nonumber
\end{align}
For endogenous non-lagged dependent explanatory variables, moment conditions analogous to Equation (\ref{EQ04:HNR-MC-linear}) result. When the non-lagged dependent explanatory variables are predetermined, one more moment condition per time period is available and for exogenous non-lagged dependent explanatory variables, all non-lagged dependent explanatory variables can be used as instruments for time periods $t=3,\dots,T$ -- compared to the case for endogenous non-lagged dependent explanatory variables.


A further set of moment conditions implied by the SA in Equation (\ref{EQ02:StdAssumpt}) is elaborated on by \citet{AhnSch1995} (hereafter AS). The authors point out that the following $T-3$ additional moment conditions can be used in estimation:
\begin{align}\label{EQ05:AS-MC-nonlinear}
E(u_{i,t} \cdot \Delta u_{i,t-1}) = 0, \qquad t = 4,\dots,T.
\end{align}
Rewriting the equation and expressing the moment conditions in terms of parameters and observable variables reveals that the AS moment conditions are nonlinear in parameters. Equations (\ref{EQ04:HNR-MC-linear}) and (\ref{EQ05:AS-MC-nonlinear}) are slightly adjusted versions of the AS-Equations (3) and (4).\footnote{The notation is adjusted to reflect the time periods of a data set. Hence, $t = 0$ of AS is changed to $t = 1$. Additionally note, that the AS moment conditions could be built on reference period $T$ instead of $t$ via $E(u_{i,T} \cdot \Delta u_{i,t}) = 0,\ \text{with}\ t = 3,\dots,T-1$ -- as originally proposed in \citet{AhnSch1995}. The HNR moment conditions could then also be expressed based on the reference period $T$ by $E(y_{i,s} \cdot \Delta u_{i,T}) = 0,\ \text{with}\ s = 1,\dots,T-2$. We adjust the AS moment conditions here, for all moment conditions to be expressed based on the same reference period.}


For a given panel data set, parameter estimates can be obtained by using the sample analogues of the moment conditions.
%This would involve observing a particular individual multiple times at time period $t$ and replacing the moment condition by the respective average for the individual. This is generally not feasible as typically, only one observation is available for each individual at each $t$. Since the moment conditions hold on average for each individual at each $t$, an alternative is to replace the moment condition at $t$ by the corresponding average over all individuals observed.
%%The sample analogues of the moment conditions result from replacing the expectation operator by the cross section average and the error terms by the vector of residuals.
This yields the $m$ sample moment conditions $\overline{\mathbf{M}} = \frac{1}{n} \sum_{i = 1}^n \mathbf{M}_i$. For the linear dynamic panel data model specified in Equation (\ref{EQ01:lin-dyn-pdm}), consider the following moment conditions and the corresponding vector of individual moment condition contributions\footnote{In the following, a tilde sign denotes the estimates during optimization, while a hat sign indicates the final optimization results (i.e., the coefficient estimates and the corresponding residuals).} $\mathbf{M}_i$ to be available for estimation:

\begin{footnotesize}
\begin{align*}
\underbrace{\left(
                 \begin{array}{c}
E(y_{i,1} \cdot \Delta u_{i,3}) \\
E(y_{i,1} \cdot \Delta u_{i,4}) \\
E(y_{i,2} \cdot \Delta u_{i,4}) \\
E(y_{i,1} \cdot \Delta u_{i,5}) \\
\vdots \\
E(y_{i,3} \cdot \Delta u_{i,5}) \\
\vdots \\
E(y_{i,T - 2} \cdot \Delta u_{i,T}) \\
\hdashline
E(x_{i,1} \cdot \Delta u_{i,3}) \\
E(x_{i,2} \cdot \Delta u_{i,3}) \\
E(x_{i,1} \cdot \Delta u_{i,4}) \\
\vdots \\
E(x_{i,3} \cdot \Delta u_{i,4}) \\
\vdots \\
E(x_{i,T - 1} \cdot \Delta u_{i,T}) \\
\hdashline
E(u_{i,4} \cdot \Delta u_{i,3}) \\
\vdots \\
E(u_{i,T} \cdot \Delta u_{i,T - 1}) \\
                 \end{array}
               \right)}_{m \times 1}
=
\left(
                 \begin{array}{c}
0 \\
0 \\
\\
\\
\vdots \\
\\
\\
0 \\
\hdashline
0 \\
\\
\\
\vdots \\
\\
\\
0 \\
\hdashline
0 \\
\vdots \\
0 \\
                 \end{array}
               \right),
\hspace{2cm} \underbrace{\mathbf{M}_i}_{m \times 1}  = \left(
                 \begin{array}{c}
y_{i, 1} \cdot \widetilde{\Delta u}_{i, 3} \\
y_{i, 1} \cdot \widetilde{\Delta u}_{i, 4} \\
y_{i, 2} \cdot \widetilde{\Delta u}_{i, 4} \\
y_{i, 1} \cdot \widetilde{\Delta u}_{i, 5} \\
\vdots \\
y_{i, 3} \cdot \widetilde{\Delta u}_{i, 5} \\
\vdots \\
y_{i, T - 2} \cdot \widetilde{\Delta u}_{i, T} \\
\hdashline
x_{i, 1} \cdot \widetilde{\Delta u}_{i, 3} \\
x_{i, 2} \cdot \widetilde{\Delta u}_{i, 3} \\
x_{i, 1} \cdot \widetilde{\Delta u}_{i, 4} \\
\vdots \\
x_{i, 3} \cdot \widetilde{\Delta u}_{i, 4} \\
\vdots \\
x_{i, T - 1} \cdot \widetilde{\Delta u}_{i, T} \\
\hdashline
\widetilde{u}_{i, 4} \cdot \widetilde{\Delta u}_{i, 3} \\
\vdots \\
\widetilde{u}_{i, T} \cdot \widetilde{\Delta u}_{i, T - 1} \\
                 \end{array}
               \right).
\end{align*}
\end{footnotesize}

The dashed lines separate the different sets of moment conditions shown here: Two sets of HNR moment conditions (derived from the lagged dependent variable and one predetermined $x_{i,t}$) and the nonlinear moment conditions. Compared to the case illustrated, $T$ moment conditions are available for each time period $t=3,\dots,T$ from the $x_{i,t}$-process if $x_{i,t}$ is exogenous -- while when $x_{i,t}$ is endogenous, one moment condition per time period is lost and the moment conditions resulting from the $x_{i,t}$ are structured equivalently to the ones that arise from the $y_{i,t}$-process\footnote{When $T$ is used as reference period (and all moment conditions involve $u_{i,T}$), the number of HNR moment conditions reduces substantially and only one moment condition is available per time period. For the nonlinear moment conditions, the number of moment conditions remains unchanged.}.


Further consider decomposing the individual moment condition contributions into $\boldsymbol{M}_i = \boldsymbol{Z}_i' \cdot \tilde{\boldsymbol{s}}_i$, where $\boldsymbol{Z}_i'$ denotes the transpose of a matrix that does not depend on parameter estimates, while the column vector $\tilde{\boldsymbol{s}}_i$ does. For the linear dynamic panel data model in Equation (\ref{EQ01:lin-dyn-pdm}) with a predetermined $x_{i,t}$, we obtain:

\begin{footnotesize}
\begin{align*}
\boldsymbol{Z}_i' = \underbrace{\left(
                  \begin{array}{cccc:cccc}
y_{i, 1} & 0        &   \cdots  & 0            &   0      & \cdots   &           &   0    \\
0        & y_{i,1}  &           &              &          &          &           &       \\
         & y_{i,2}  &           &              &          &          &           &       \\
         & 0        &           &              &          &          &           &       \\
\vdots   & \vdots   &           & \vdots       & \vdots   &          &           & \vdots  \\
         &          &           &              &          &          &           &       \\
         &          &           & 0            &          &          &           &       \\
         &          &           & y_{i,1}      &          &          &           &       \\
         &          &           & \vdots       &          &          &           &       \\
0        & 0        &           & y_{i,T-2}    &   0      & \cdots   &           &   0    \\
\hdashline
x_{i,1}  & 0        &   \cdots  & 0            &   0      & \cdots   &           &   0    \\
x_{i,2}  & 0        &           &              &          &          &           &       \\
0        & x_{i,1}  &           &              &          &          &           &       \\
         & x_{i,2}  &           &              &          &          &           &  \\
         & x_{i,3}  &           &              &          &          &           &       \\
         & 0        &           &              &          &          &           &       \\
\vdots   & \vdots   &           & \vdots       & \vdots   &          &           & \vdots  \\
         &          &           &              &          &          &           &       \\
         &          &           & 0            &          &          &           &       \\
         &          &           & x_{i,1}      &          &          &           &       \\
         &          &           & \vdots       &          &          &           &       \\
0        & 0        &           & x_{i, T - 1} &   0      & \cdots   &           &   0    \\
\hdashline
0        &          &   \cdots  & 0            & 1        &  0       & \cdots    &   0 \\
\vdots   &          &           & \vdots       & \vdots   & \ddots   &           &   \vdots \\
         &          &           &              &          &          &           &   0 \\
0        &          &   \cdots  & 0            & 0        & \cdots   &  0        &   1 \\
                  \end{array}
                \right)}_{m \times (2T - 5)}
, \hspace{0.5cm} \tilde{\boldsymbol{s}}_i = \underbrace{\left(
                                        \begin{array}{c}
                                          \widetilde{\Delta u}_{i, 3} \\
                                          \widetilde{\Delta u}_{i, 4} \\
                                          \vdots \\
                                          \widetilde{\Delta u}_{i, T} \\
                                          \hdashline
                                          \widetilde{u}_{i, 4} \cdot \widetilde{\Delta u}_{i, 3} \\
                                          \widetilde{u}_{i, 5} \cdot \widetilde{\Delta u}_{i, 4} \\
                                          \vdots \\
                                          \widetilde{u}_{i, T} \cdot \widetilde{\Delta u}_{i, T - 1} \\
                                        \end{array}
                                      \right)}_{(2T - 5) \times 1}.
\end{align*}
\end{footnotesize}

Compared to the case shown, $x_{i,1}, \dots, x_{i,T}$ can be used for each time period $t=3,\dots,T$ in the HNR-part of the matrix $\boldsymbol{Z}_i'$ if $x_{i,t}$ is strictly exogenous --  while an equivalent structure to the $y_{i,t}$-part results for the $x_{i,t}$-part of the matrix if $x_{i,t}$ is endogenous. Changing the reference period from $t$ to $T$ reduces the HNR-part of the matrix to a column vector; the structure of the AS-part remains unchanged.

Stacking the $\boldsymbol{Z}_i'$ for all cross sectional observations horizontally yields the $m \times n(2T - 5)$ matrix $\boldsymbol{Z}' = (\boldsymbol{Z}_1', \dots, \boldsymbol{Z}_n')$. Concatenating the column vectors $\tilde{\boldsymbol{s}}_i$ yields the $n(2T - 5)$ vector $\tilde{\boldsymbol{s}}$.





\subsection{Moment conditions from extended assumptions} \label{sec:ExtAssumpt}
Under SA, the moment conditions stated in Equations (\ref{EQ04:HNR-MC-linear}), (\ref{EQ06:HNR-MC-linear-x_it}), and (\ref{EQ05:AS-MC-nonlinear}) can be employed to obtain coefficient estimates. Additional moment conditions can be derived from the assumption
\begin{equation} \label{EQ07:CCE}
E(\Delta y_{i,t} \cdot \eta_i) = 0, \quad i=1,\dots,n.
\end{equation}
This expression requires that the dependent variable and the unobserved individual-specific effects are constantly correlated over time for each individual. Deviations from the assumption are required to be unsystematic over both, the cross section and the time series dimension \citep[see Section 6.5 in][which also provides an example]{Are2003}. For the case of non-lagged dependent explanatory variables, \citet{BluBonWin2001} state that if $\Delta y_{i,t}$ and $\eta_i$ are correlated, it is still possible that $\Delta x_{i,t}$ and $\eta_i$ are uncorrelated -- while the reverse is unlikely to be the case \citep[for a derivation confirming this statement see][]{Fri2019}.


From the `constant correlated effects'\footnote{\citet{BunSar2015} use this term and point out that this assumption is also referred to as `effect stationarity' \citep[][]{Kiv2007a} or `mean stationarity' \citep[][]{Are2003} in the literature.} assumption, the additional $T-2$ \citet{AreBov1995} (hereafter AB) linear moment conditions can be derived:
\begin{align} \label{EQ08:AB-MC-linear_yit}
E(\Delta y_{i,t-1} \cdot u_{i,t}) = 0, \quad t=3,\dots,T.
\end{align}
By rewriting these moment conditions, it can be shown that the AB moment conditions encompass the nonlinear AS moment conditions and render them redundant for estimation \citep[for a derivation see][]{Fri2019}.

Additional AB moment conditions can be derived from the non-lagged dependent explanatory variables. Depending on the nature of the $x_{i,t}$-process, the further AB moment conditions are available for estimation:
\begin{align*}
E(\Delta x_{i,v} \cdot u_{i,t}) = 0, \qquad & \text{where} \\
& v = 2,\dots,t-1;\ t=3, \dots T, \quad \text{for} \quad x=x^{end}, \\
& v = t;\ t=2,\dots,T, \quad \text{for} \quad x = x^{ex} \quad \text{or} \quad x = x^{pre}.
\end{align*}
From an endogenous $x_{i,t}$, $T-2$ moment conditions can be derived -- while $T-1$ moment conditions are available for an exogenous or predetermined $x_{i,t}$.

When using the HNR and AB moment conditions to estimate the linear dynamic panel data model in Equation (\ref{EQ01:lin-dyn-pdm}) with a predetermined explanatory variable, $\boldsymbol{M}_i$ is as follows:

\begin{footnotesize}
\begin{align*}
\boldsymbol{Z}_i' & = \underbrace{\left(
                  \begin{array}{cccc:cccc:cccc}
y_{i, 1} & 0        & \cdots   & 0            &   0             & \cdots          &           & 0                   &  0              & \cdots          &        & 0               \\
0        & y_{i, 1} &          &              &                 &                 &           &                     &                 &                 &        &                 \\
         & y_{i, 2} &          &              &                 &                 &           &                     &                 &                 &        &                 \\
\vdots   & 0        &          & \vdots       & \vdots          &                 &           & \vdots              & \vdots          &                 &        & \vdots          \\
         &          &          &              &                 &                 &           &                     &                 &                 &        &                 \\
         &          &          & 0            &                 &                 &           &                     &                 &                 &        &                 \\
         &          &          & y_{i,1}      &                 &                 &           &                     &                 &                 &        &                 \\
         &          &          & \vdots       &                 &                 &           &                     &                 &                 &        &                 \\
0        & \cdots   & 0        & y_{i, T - 2} &   0             & \cdots          &           & 0                   & 0               & \cdots          &        & 0               \\
\hdashline
x_{i, 1} &   0      & \cdots   & 0            &   0             & \cdots          &           & 0                   & 0               & \cdots          &        & 0               \\
x_{i, 2} &   0      &          &              &                 &                 &           &                     &                 &                 &        &                 \\
0        & x_{i, 1} &          &              &                 &                 &           &                     &                 &                 &        &                 \\
         & x_{i, 2} &          &              &                 &                 &           &                     &                 &                 &        &                 \\
         & x_{i, 3} &          &              &                 &                 &           &                     &                 &                 &        &                 \\
\vdots   &   0      &          & \vdots       & \vdots          &                 &           & \vdots              & \vdots          &                 &        & \vdots          \\
         &          &          &              &                 &                 &           &                     &                 &                 &        &                 \\
         &          &          & 0            &                 &                 &           &                     &                 &                 &        &                 \\
         &          &          & x_{i,1}      &                 &                 &           &                     &                 &                 &        &                 \\
         &          &          & \vdots       &                 &                 &           &                     &                 &                 &        &                 \\
0        & \cdots   & 0        & x_{i, T - 1} &   0             & \cdots          &           & 0                   & 0               & \cdots          &        & 0               \\
\hdashline
0        &          & \cdots   & 0            & \Delta y_{i, 2} & 0               & \cdots    & 0                   & 0               & \cdots          &        & 0               \\
\vdots   &          &          & \vdots       & 0               & \Delta y_{i, 3} &           & \vdots              & \vdots          &                 &        & \vdots          \\
         &          &          &              & \vdots          &                 & \ddots    & 0                   &                 &                 &        &                 \\
0        &          & \cdots   & 0            & 0               & \cdots          & 0         & \Delta y_{i, T - 1} & 0               & \cdots          &        & 0               \\
\hdashline
0        &          & \cdots   & 0            & 0               & \cdots          &           & 0                   & \Delta x_{i, 2} & 0               & \cdots & 0               \\
         &          &          &              &                 &                 &           &                     & 0               & \Delta x_{i, 3} &        & \vdots          \\
\vdots   &          &          & \vdots       & \vdots          &                 &           & \vdots              & \vdots          &                 & \ddots & 0               \\
0        &          & \cdots   & 0            & 0               & \cdots          &           & 0                   & 0               & \cdots          & 0      & \Delta x_{i, T} \\
                  \end{array}
                \right)}_{m \times (3T-5)}
,\\
\\
\tilde{\boldsymbol{s}}_i' & = \underbrace{\begin{array}{cccc:cccc:cccc}
(\widetilde{\Delta u}_{i, 3}, & \widetilde{\Delta u}_{i, 4}, & \cdots, & \widetilde{\Delta u}_{i, T} & \widetilde{u}_{i, 3}, & \widetilde{u}_{i, 4}, & \cdots, & \widetilde{u}_{i, T}, & \widetilde{u}_{i, 2}, & \widetilde{u}_{i, 3}, & \cdots, & \widetilde{u}_{i, T})
                            \end{array} }_{1 \times (3T-5)}.
\end{align*}
\end{footnotesize}

Compared to the case shown, no additional AB moment conditions arise if $x_{i,t}$ is strictly exogenous. When $x_{i,t}$ is endogenous, the structure of the $x_{i,t}$-part corresponding to the AB moment conditions of $\boldsymbol{Z_i'}$ reduces by the last element and is equivalent to the $y_{i,t}$-part corresponding to the AB moment conditions. The changes of the HNR-part of the matrix are as illustrated in Section \ref{sec:StdAssumpt}. Changing the reference period from $t$ to $T$ yields the changes in the number of HNR moment conditions discussed earlier; the number of AB moment conditions remains the same, with the structure of the AB-part of the matrix $\boldsymbol{Z}_i'$ reducing to a column vector\footnote{When using the reference period $T$ instead of $t$, the AB moment conditions can be built on
\begin{align*}
E(\Delta y_{i,v} \cdot u_{i,T}) = 0, \qquad & \text{with} \qquad t = 3, \dots ,T, \\
E(\Delta x_{i,v} \cdot u_{i,T}) = 0, \qquad & \text{where} \\
& v = 2, \dots ,T-1, \quad \text{for} \quad x = x^{end}, \\
& v = 2, \dots ,T, \quad \text{for} \quad x = x^{ex} \quad \text{or} \quad x = x^{pre}.
\end{align*}
}.


%
%In case of an exogenous $x_{i,t}$, $T$ HNR moment conditions are available for each time period $t=3,\dots,T$ for the non-lagged dependent part of $\boldsymbol{Z}_i'$ which results from equations in differences -- while no additional AB moment conditions can be derived from the equations in levels. Compared to the case shown above, one moment condition is lost for every time period in the non-lagged dependent part of $\boldsymbol{Z}_i'$ which is attributable to the HNR moment conditions, if an endogenous $x_{i,t}$ is included instead of a predetermined one. One additional moment condition is lost for the $\Delta x_{i,t}$-part representing the AB moment conditions as well, when replacing the predetermined $x_{i,t}$ by an endogenous one. Changing the reference period from $t$ to $T$ yields the changes in the number of HNR moment conditions discussed earlier; the number of AB moment conditions remains the same, with the structure of the AB part of the matrix $\boldsymbol{Z}_i'$ reducing to a column.
%














%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{GMM estimation} \label{sec:gmm}

\subsection{Minimization criterion}
For a given loss function, the $p$ parameters of the linear dynamic panel data model in Equation (\ref{EQ01:lin-dyn-pdm}) can be estimated by employing the moment conditions derived from the model assumptions. According to the necessary (but not sufficient) order condition for identification \citep[see, e.g.,][]{Hay2000econometrics}, at least as many moment conditions need to be available as there are model parameters for the parameters to be estimable\footnote{A discussion of the assumptions required for identification, consistency, and asymptotic normality of the GMM estimator when estimating linear dynamic panel data models is provided in \citet{Fri2019}.}. Each moment condition is a function of the $p$ model parameters. If the number of moment conditions and the number of model parameters coincide ($m = p$), the system of equations (or moment conditions) possesses a unique solution. Due to the number of moment conditions increasing with the time series dimension, the number of available moment conditions typically exceeds the number of model parameters with linear dynamic panel data models. Therefore, obtaining parameter estimates from the system of equations defined by the moment conditions requires an aggregation scheme such as the generalized method of moments (GMM). For a given sample, GMM estimation minimizes the aggregated squared distance of the moment conditions from zero and can be represented as
%\citep[see, e.g.,][p.202]{Hay2000econometrics}

\begin{align}\label{EQ09:Objective-function-GMM}
L_{\boldsymbol{W}}^{2} = \overline{\boldsymbol{M}}' \cdot \boldsymbol{W} \cdot \overline{\boldsymbol{M}}.
\end{align}
The index of the $L_{\boldsymbol{W}}^{2}$-norm expresses that the norm depends on the weighting matrix $\boldsymbol{W}$ and the superscript indicates that the norm is a quadratic form. The $m \times m$ weighting matrix $\boldsymbol{W}$ guides the aggregation of the $m$ moment conditions. Recall the notation developed in Section \ref{sec:ldpdm}, where the moment conditions are decomposed into a vector that depends on the parameter estimates $\tilde{\boldsymbol{s}}$ and a matrix $\boldsymbol{Z}$ that does not. Plugging these two terms into Equation (\ref{EQ09:Objective-function-GMM}) gives:
\begin{align*}
L_{\boldsymbol{W}}^{2} = \frac{1}{n^2} \cdot \tilde{\boldsymbol{s}}' \boldsymbol{Z} \cdot \boldsymbol{W} \cdot \boldsymbol{Z}' \tilde{\boldsymbol{s}}.
\end{align*}
Minimizing the equation yields the GMM estimator $\hat{\boldsymbol{\theta}}$.















\subsection{One-step, two-step, and multiple-step estimation} \label{section:1_2_step}
In practice, GMM estimation is frequently carried out in multiple steps. In order to start the estimation process, an initial estimate of the weighting matrix $\widehat{\boldsymbol{W}}$ is required. Obviously, plugging in different weighting matrices into Equation (\ref{EQ09:Objective-function-GMM}) yields varying objective function values and different estimates for the model parameters.
Different propositions for the first step weighting matrix -- with varying asymptotic efficiency -- exist in the literature \citep[see][]{BluBonWin2001} for the various types of moment conditions which can be employed in the estimation of the linear dynamic panel data model in Equation (\ref{EQ01:lin-dyn-pdm}). Common examples involve identity or tridiagonal matrices. Generally, the proposed weighting matrices are based on the expected variances and covariances of the moment conditions and are derived from the underlying model assumptions. Assuming consistency and asymptotic normality of the GMM estimator, the optimal $\boldsymbol{W}$ is proportional (up to a multiplicative constant) to the inverse of the variance covariance matrix of the moment conditions \citep[see, e.g.,][]{Are2003}. A possible estimate for the first step weighting matrix $\widehat{\boldsymbol{W}}_1$ of the one-step GMM estimator (GMM1S) is
\begin{align} \label{EQ10:W1step}
\widehat{\boldsymbol{W}}_1 & = \left(\frac{1}{n} \cdot \boldsymbol{Z}' \boldsymbol{H} \boldsymbol{Z}\right)^{-1}.
\end{align}
The structure of the matrix $\boldsymbol{H}$ varies depending on the types of moment conditions employed in estimation. When only HNR moment conditions are used, \citet{AreBon1991} propose to set the matrix to

\begin{align*}
\boldsymbol{H}_{HNR} & = \left(
\begin{array}{cccccc}
2       &   -1      &   0       &   0       &   \dots   &   0       \\
-1      &   2       &   -1      &   0       &           &           \\
0       &   -1      &   \ddots  &   \ddots  &   \ddots  &   \vdots  \\
\vdots  &   \ddots  &   \ddots  &           &           &   0       \\
        &           &           &           &   2       &   -1      \\
0       &           &   \dots   &   0       &   -1      &   2
\end{array}
\right).
\end{align*}

The tridiagonal matrix $\boldsymbol{H}_{HNR}$ accounts for the serial correlation in the idiosyncratic remainder components introduced by first differencing Equation (\ref{EQ01:lin-dyn-pdm}) to eliminate the unobserved individual-specific effects from the equation.

When using only the AB moment conditions in estimation, a choice for $\boldsymbol{H}$ often encountered in practice is the identity matrix with $T-2$ diagonal elements
\begin{align*}
\boldsymbol{H}_{AB} & = \left(
\begin{array}{cccccc}
1       &   0       &   \dots   &       &       &   0       \\
0       &   1       &           &       &       &           \\
\vdots  &           &   \ddots  &       &       &   \vdots  \\
        &           &           &       &       &   0       \\
0       &           &   \dots   &       &   0   &   1
\end{array}
\right).
\end{align*}

For the AS moment conditions, an identity matrix with $T-3$ diagonal elements is frequently used as $\boldsymbol{H}_{AS}$ \citep[see, e.g.,][]{BluBonWin2001,Kri2019}.

Finally, when two different sets of moment conditions are employed, a general representation of $\boldsymbol{H}$ is
\begin{align*}
\boldsymbol{H} & = \left(
\begin{array}{cc}
\boldsymbol{A}      & \boldsymbol{B}    \\
\boldsymbol{B}'      & \boldsymbol{C}    \\
\end{array}
 \right),
\end{align*}

where the matrices $\boldsymbol{A}$, $\boldsymbol{B}$, and $\boldsymbol{C}$ are chosen depending on the particular moment conditions employed in GMM estimation.
Note that $\boldsymbol{A}$ and $\boldsymbol{C}$ correspond to the expected variance covariance properties within a set of moment conditions, while $\boldsymbol{B}$ corresponds to the expected covariance across the different sets of moment conditions and $\boldsymbol{B}'$ is the transpose.


An estimate for the weighting matrix $\widehat{\boldsymbol{W}}_2$ of the two-step GMM estimator (GMM2S) is
\begin{align} \label{EQ11:W2step}
\widehat{\boldsymbol{W}}_2 & = \left(\frac{1}{n} \cdot \boldsymbol{Z}' \hat{\boldsymbol{s}}_{1} \hat{\boldsymbol{s}}_{1}' \boldsymbol{Z}\right)^{-1},
\end{align}
where $\hat{\boldsymbol{s}}_{1}$ denotes the residuals from one-step estimation. When the nonlinear moment conditions are used, nonlinear optimization techniques are required to obtain coefficient estimates. Per default, GMM estimation by \pkg{pdynmc} is based on numerical optimization. To initialize the optimization procedure, starting values are drawn for all parameter estimates from a uniform distribution over the interval [-1, 1].
%Multistarting can be employed to avoid local minima when computing the GMM1S estimates, where the number of multistarts is adjustable by the user.
%Per default no multistarting is carried out.
%By default, GMM1S is calculated for 3 different parameter starting value combinations. The number of multistarts can be adjusted by the user.
GMM2S then employs the parameter estimates obtained by GMM1S as starting values. %and performs no multistarting.
For the optimization procedure, we rely on the \proglang{R}-package \pkg{optimx} \citep{NasVar2011,Nas2014}. All optimization routines implemented in \pkg{optimx} are available in \pkg{pdynmc}. From our experience, the Variable Metric method \citep{Fle1970new,Nas1990compact} seems to work satisfactory in the estimation of linear dynamic panel data models. In all settings encountered while programming the package, the results from this method were close to closed form results for GMM estimation based on linear moment conditions.
The Variable Metric method is named \code{BFGS} in \pkg{optimx}\footnote{For more details and references on the available optimization methods in \pkg{optimx} see the package documentation and \citet{NasVar2011}.} and serves as the default procedure in \pkg{pdynmc}.
%This may, however, change in future versions of \pkg{pdynmc} depending on prospective results and insights -- this is especially the case for the nonlinear moment conditions, which cannot be compared to closed form results.
Note that for GMM estimation based on linear moment conditions, the closed form results are computed and stored along with the optimization results.


Alternatively to one-step and two-step procedures, GMM estimation can be carried out with the continuously updating estimator (GMMCU). The GMMCU is an iterative procedure, where the weighting matrix, the corresponding parameter estimates, and the residuals are updated until either one of two stopping criteria is attained: The procedure stops, when the change in coefficient estimates from one estimation step to the next does not exceed a certain pre-specified threshold $z_{\text{tol}}$. Otherwise, GMMCU stops after a pre-specified number of maximum iterations $h_{\text{iter}}$.
Asymptotically, one-step, two-step, and multiple-step GMM estimation are equivalent -- though, differences occur in finite samples and Monte Carlo evidence exists that the finite sample performance may improve when using GMMCU \citep[see, e.g.,][]{HanHeaYar1996}. In \pkg{pdynmc} all three different estimation procedures are available.





\subsection{Closed form solution}
When estimating the linear dynamic panel data model in Equation (\ref{EQ01:lin-dyn-pdm}) by GMM based on linear moment conditions only, numerical optimization methods are not required to obtain coefficient estimates. One-step estimates $\hat{\boldsymbol{\theta}}_{1}$ are available from:
\begin{align} \label{EQ12:closedForm1step}
\hat{\boldsymbol{\theta}}_{1} = (\boldsymbol{X}' \boldsymbol{Z} \widehat{\boldsymbol{W}}_{1} \boldsymbol{Z}' \boldsymbol{X})^{-1} \boldsymbol{X}' \boldsymbol{Z} \widehat{\boldsymbol{W}}_{1} \boldsymbol{Z}' \boldsymbol{y}.
\end{align}
The matrix $\boldsymbol{X}$ contains all right-hand side variables\footnote{Note that -- depending on the moment conditions employed in estimation -- all matrices and vectors given in the following may contain observations in levels and/or first differences.} from Equation (\ref{EQ01:lin-dyn-pdm}) and $\widehat{\boldsymbol{W}}_{1}$ is the estimated one-step weighting matrix from Equation (\ref{EQ10:W1step}). In order to calculate the two-step coefficient estimates $\hat{\boldsymbol{\theta}}_{2}$, $\widehat{\boldsymbol{W}}_{1}$ needs to be replaced by the estimated two-step weighting matrix $\widehat{\boldsymbol{W}}_{2}$:
\begin{align} \label{EQ13:closedForm2step}
\hat{\boldsymbol{\theta}}_{2} = (\boldsymbol{X}' \boldsymbol{Z} \widehat{\boldsymbol{W}}_{2} \boldsymbol{Z}' \boldsymbol{X})^{-1} \boldsymbol{X}' \boldsymbol{Z} \widehat{\boldsymbol{W}}_{2} \boldsymbol{Z}' \boldsymbol{y}.
\end{align}
Two comments on the closed form expressions may be helpful here. {First}, recall from Equations (\ref{EQ10:W1step}) and (\ref{EQ11:W2step}) that the one-step- and two-step weighting matrices depend on the number of cross sectional units in the data set $n$. Considering the closed form for the coefficient estimates in greater detail reveals that for Equations (\ref{EQ12:closedForm1step}) and (\ref{EQ13:closedForm2step}), the factor $n$ cancels from both expressions. {Second}, as mentioned in the previous section, updating the weighting matrix and computing coefficient estimates does not necessarily have to stop at the second step. The procedure can be iterated until either one of the stopping criteria is reached.

















\section{Standard errors and inference}


\subsection{Standard errors} \label{sec:covariance_matrix}
Asymptotic one-step standard errors for the estimated coefficients can be obtained by taking the square root of the main diagonal elements of the estimated one-step variance covariance matrix
\begin{align} \label{EQ14:VCov_beta_1step}
\widehat{\boldsymbol{\Omega}}(\hat{\boldsymbol{\theta}}_{1}) = n \cdot (\boldsymbol{X}' \boldsymbol{Z} \widehat{\boldsymbol{W}}_{1} \boldsymbol{Z}' \boldsymbol{X})^{-1} \hat{\sigma}_{1}^2, \qquad \text{with} \qquad \hat{\sigma}_{1}^2 = \hat{\boldsymbol{s}}_{1}'\hat{\boldsymbol{s}}_{1} \cdot \frac{1}{N - p}.
\end{align}
In the formula, $N$ is the number of observations available for estimation (i.e., the cross section dimension times the time series dimension minus the number of missing observations), $p$ denotes the number of estimated coefficients, and $\hat{\boldsymbol{s}}_{1}$ are residuals from GMM1S \citep[see][]{DooAreBon2012dpd}. As stated in \citet{Win2005}, robust one-step standard errors are available from
\begin{align} \label{EQ15:VCov_beta_1step_rob}
\widehat{\boldsymbol{\Omega}}_{r}(\hat{\boldsymbol{\theta}}_{1}) = n \cdot & (\boldsymbol{X}' \boldsymbol{Z} \widehat{\boldsymbol{W}}_{1} \boldsymbol{Z}' \boldsymbol{X})^{-1} \boldsymbol{X}' \boldsymbol{Z} \widehat{\boldsymbol{W}}_{1} \widehat{\boldsymbol{W}}_{2}^{-1} \widehat{\boldsymbol{W}}_{1} \boldsymbol{Z}' \boldsymbol{X} \cdot \\
& (\boldsymbol{X}' \boldsymbol{Z} \widehat{\boldsymbol{W}}_{1} \boldsymbol{Z}' \boldsymbol{X})^{-1}, \nonumber
\end{align}
while asymptotic two-step standard errors can be computed from
\begin{align} \label{EQ16:VCov_beta_2step}
\widehat{\boldsymbol{\Omega}}(\hat{\boldsymbol{\theta}}_{2}) = n \cdot (\boldsymbol{X}' \boldsymbol{Z} \widehat{\boldsymbol{W}}_{2} \boldsymbol{Z}' \boldsymbol{X})^{-1}.
\end{align}
Since asymptotic two-step GMM standard errors for the estimated coefficients exhibit a downward bias in small samples, they can, however, be substantially lower than one-step GMM standard errors \citep[see, e.g.,][]{AreBon1991}. \citet{Win2005} relates the bias to the dependence of the two-step weighting matrix on parameter estimates (the one-step estimates) and proposes an analytic correction of the two-step standard errors based on a first order Taylor-series expansion:
\begin{align} \label{EQ17:WindmeijerCorrSE}
\widehat{\boldsymbol{\Omega}}_c (\hat{\boldsymbol{\theta}}_{2}) = & \boldsymbol{F} + \boldsymbol{D}_{\hat{\boldsymbol{\theta}}_{2}, \widehat{\boldsymbol{W}}_{2}} \boldsymbol{F} + \boldsymbol{F} \boldsymbol{D}_{\hat{\boldsymbol{\theta}}_{2}, \widehat{\boldsymbol{W}}_{2}}' \\
& + \boldsymbol{D}_{\hat{\boldsymbol{\theta}}_{2}, \widehat{\boldsymbol{W}}_{2}} \widehat{\boldsymbol{\Omega}}_r(\hat{\boldsymbol{\theta}}_{1}) \boldsymbol{D}_{\hat{\boldsymbol{\theta}}_{2}, \widehat{\boldsymbol{W}}_{2}}', \nonumber
\end{align}
where the expression $\boldsymbol{F}$ is defined as
\begin{align*}
\boldsymbol{F} = n \cdot (\boldsymbol{X}' \boldsymbol{Z} \widehat{\boldsymbol{W}}_{2} \boldsymbol{Z}' \boldsymbol{X})^{-1}.
\end{align*}
This expression is equivalent to the estimated uncorrected two-step variance covariance matrix of the coefficient estimates $\widehat{\boldsymbol{\Omega}}(\hat{\boldsymbol{\theta}}_{2})$ in Equation (\ref{EQ16:VCov_beta_2step}). The computation of the correction $\boldsymbol{D}_{\hat{\boldsymbol{\theta}}_{2}, \widehat{\boldsymbol{W}}_{2}}$ is involved when multiple parameters are estimated. For a single parameter, it equals
\begin{align*}
\boldsymbol{D}_{\hat{\boldsymbol{\theta}}_{2}, \widehat{\boldsymbol{W}}_{2}} = - \frac{1}{n} \cdot \boldsymbol{F} \boldsymbol{X}' \boldsymbol{Z} \widehat{\boldsymbol{W}}_{2} \left. \frac{\partial \widehat{\boldsymbol{W}}^{-1}(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \right\rvert_{\boldsymbol{\theta} = \hat{\boldsymbol{\theta}}_{1}} \widehat{\boldsymbol{W}}_{2} \boldsymbol{Z}' \hat{\boldsymbol{s}}_{2}.
\end{align*}
The vector $\hat{\boldsymbol{s}}_{2}$ denotes the two-step residuals and the first derivative of the weighting matrix for two-step GMM estimation evaluated at $\hat{\boldsymbol{\theta}}_{1}$ can be calculated from
\begin{align*}
\left. \frac{\partial \widehat{\boldsymbol{W}}^{-1}(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \right\rvert_{\boldsymbol{\theta} = \hat{\boldsymbol{\theta}}_{1}} = -\frac{1}{n} \cdot \boldsymbol{Z}' (\boldsymbol{X} \hat{\boldsymbol{s}}_{1}' + \hat{\boldsymbol{s}}_{1} \boldsymbol{X}') \boldsymbol{Z}.
\end{align*}
Two remarks on the computation of the standard errors might be helpful here: {First,} similar to the computation of the closed form expressions for the coefficient estimates, the formulas in the Equations (\ref{EQ15:VCov_beta_1step_rob}), (\ref{EQ16:VCov_beta_2step}), and (\ref{EQ17:WindmeijerCorrSE}) do not depend on the number of cross sectional units in the data set $n$, as the term cancels out with the $1/n$ from $\widehat{\boldsymbol{W}}_1$ and $\widehat{\boldsymbol{W}}_2$. Equation (\ref{EQ14:VCov_beta_1step}) depends on the total number of observations available in the data set by the degrees of freedom correction in the calculation of $\hat{\sigma}_{1s}^2$. This affects the calculation of the standard errors when there are missing observations.
{Second,} note that an alternative to the analytic correction of the two-step standard errors proposed by \citet{AreBon1991} is to replace the standard errors of the second estimation step by those from the first step.






\subsection{Specification testing} \label{sec:specTesting}
\citet{AreBon1991} suggest a test for second order serial correlation in the idiosyncratic remainder components. The test is generalized to higher orders $j$ by \citet{Are2003} and can be used as a specification test in the estimation of linear dynamic panel data models. The reasoning is that, although, first order serial correlation is present in the idiosyncratic remainder components for GMM estimation based on first differenced equations\footnote{First order serial correlation in the $\varepsilon_{i,t}$ is introduced by first differencing. Even when the $\varepsilon_{i,t}$ in levels are i.i.d., the first differenced $\varepsilon_{i,t}$ are correlated \citep[for a derivation see, e.g.,][]{Fri2019}.}, no higher order autocorrelation should prevail. The serial correlation test of \citet{AreBon1991} boils down to checking if the deviation of the covariance of the residuals of period $t$ with the residuals of period $t-j$ from zero is large enough to indicate that $j$-th order serial correlation might be present in the idiosyncratic remainder components. The null hypothesis of the test is that there is no serial correlation in the $\varepsilon_{i,t}$. The corresponding test statistics are defined as
\begin{align*}
T_{m_j} = \frac{\hat{r}_j}{\hat{\sigma}_{\hat{r}_j}}, \qquad \text{with} \qquad T_{m_j} \overset{a}{\sim} \mathcal{N}(0,1),
\end{align*}
where $\hat{\sigma}_{\hat{r}_j}$ is the standard error of the $j$-th order autocovariance of the residuals $\hat{r}_j$. For the linear dynamic panel data model specified in Equation (\ref{EQ01:lin-dyn-pdm}), this autocovariance of the residuals is the sample equivalent to
\begin{align*}
r_j = \frac{1}{T-3-j} \cdot \sum_{t=4+j}^{T} r_{t,j}, \qquad \text{with} \qquad r_{t,j} = E(\Delta s_{i,t} \Delta s_{i,t-j}),
\end{align*}
the average $j$-th order autocovariance of the idiosyncratic remainder components \citep[see][]{Are2003}. As detailed by \citet{AreBon1991} and \citet{DooAreBon2012dpd}, the corresponding scaled autocovariance of the residuals can be calculated by
\begin{align*}
\hat{r}_{t,j} = \frac{1}{\sqrt{n}} \cdot \hat{\boldsymbol{s}}_{t}' \ \hat{\boldsymbol{s}}_{t-j},
\end{align*}
where $\hat{\boldsymbol{s}}_{t}$ and $\hat{\boldsymbol{s}}_{t-j}$ are column vectors which contain the residuals from one-step, two-step, or multiple-step GMM estimation for all cross sectional units at the respective time period; the index at $\hat{\boldsymbol{s}}_{t-j}$ indicates that the corresponding residuals are lagged $j$ time periods. According to \citet{AreBon1991}, the estimated variance of the $j$-th order autocovariance of the residuals is available from
\begin{align*}
\hat{\sigma}_{\hat{r}_j}^2 = \frac{1}{n} \cdot & \hat{\boldsymbol{s}}_{t-j}' \ \widehat{\boldsymbol{\Omega}}({\hat{\boldsymbol{s}}}) \hat{\boldsymbol{s}}_{t-j} - 2 \cdot \hat{\boldsymbol{s}}_{t-j}' \ \boldsymbol{X}(\boldsymbol{X}' \boldsymbol{Z} \widehat{\boldsymbol{W}} \boldsymbol{Z}' \boldsymbol{X})^{-1} \boldsymbol{X}' \boldsymbol{Z} \widehat{\boldsymbol{W}} \boldsymbol{Z}' \widehat{\boldsymbol{\Omega}}({\hat{\boldsymbol{s}}}) \hat{\boldsymbol{s}}_{t-j} \ + \\
& \hat{\boldsymbol{s}}_{t-j}' \ \boldsymbol{X} \widehat{\boldsymbol{\Omega}}(\hat{\boldsymbol{\theta}}) \boldsymbol{X}' \hat{\boldsymbol{s}}_{t-j}.
\end{align*}
Note that the vectors of residuals $\hat{\boldsymbol{s}}$, $\hat{\boldsymbol{s}}_{-j}$ and the matrices $\widehat{\boldsymbol{W}}$, $\widehat{\boldsymbol{\Omega}}({\hat{\boldsymbol{s}}})$, and $\widehat{\boldsymbol{\Omega}}(\hat{\boldsymbol{\theta}})$ depend on the actual estimation step and the latter two matrices also depend on the estimated type of variance covariance matrix (i.e., robust or asymptotic for one-step estimation; Windmeijer-corrected or asymptotic for two-step estimation). All corresponding indices are dropped here in order to provide one general formula instead of the four specific ones.








\subsection{Overidentifying restrictions testing} \label{sec:oirTesting}
When the system of equations from which the model parameters are estimated by GMM is overidentified (i.e., when the number of moment conditions exceeds the number of parameters to be estimated), it is possible to assess the validity of the overidentifying restrictions by the Sargan test \citep{Sar1958estimation}. The presumed null hypothesis is that the overidentifying restrictions are valid.
According to \citet{AreBon1991} and \citet{DooAreBon2012dpd}, the test statistic of the Sargan test can be computed from
\begin{align*}
T_S = n \cdot \hat{\boldsymbol{s}}_{1}' \boldsymbol{Z} \widehat{\boldsymbol{W}}_1 \boldsymbol{Z}' \hat{\boldsymbol{s}}_{1} \cdot \hat{\sigma}_{1}^{-2}, \quad \text{with} \quad T_S \overset{a}{\sim} \chi^2(m - p).
\end{align*}
Under suitable conditions, which ensure asymptotic normality of the GMM estimator\footnote{For Theorems, Propositions, and extensive discussions for GMM estimators see \citet{NewMcF1994,Hay2000econometrics}. A discussion of GMM estimation of linear dynamic panel data models and the underlying assumptions is provided by \citet{Fri2019}.} and the additional assumption of conditional homoscedasticity, the test statistic is asymptotically $\chi^2$-distributed with $m-p$ degrees of freedom; $m$ equals the number of instruments employed in estimation \citep[see, e.g.,][]{Hay2000econometrics}.
%\citep[see, e.g.,][p.227-228]{Hay2000econometrics}.

An alternative test statistic -- where a finite fourth moments assumption is imposed instead of conditional homoscedasticity -- is the $J$-test \citep{Han1982large}. The $J$-test statistic results from replacing $\widehat{\boldsymbol{W}}_{1}$ in the above formula by $\widehat{\boldsymbol{W}}_{2}$, the one-step residuals by the two-step residuals, and dropping the multiplication with $\hat{\sigma}_{1}^{-2}$:
\begin{align*}
T_J = n \cdot \hat{\boldsymbol{s}}_{2}' \boldsymbol{Z} \widehat{\boldsymbol{W}}_{2} \boldsymbol{Z}' \hat{\boldsymbol{s}}_{2}, \quad \text{with} \quad T_J \overset{a}{\sim} \chi^2(m - p).
\end{align*}
The idea underlying the test statistics $T_S$ and $T_J$ is, that when the moment conditions are valid, the sample analogues of these conditions should be close to zero. A large value of the test statistic indicates that some of the moment conditions may be invalid, that some of the model assumptions may be incorrect, or both \citep[see, e.g.,][]{Hay2000econometrics}.
%\citep[see, e.g.,][p.217-218]{Hay2000econometrics}.


Variations of the two tests allow to check the validity of subsets of moment conditions. These tests are also referred to as `difference-in-Hansen'/`difference-in-Sargan' tests \citep[see, e.g.,][]{Roo2009note}, `incremental Hansen'/`incremental Sargan' tests \citep[see, e.g.,][]{Are2003}, or $C$-statistics \citep[see, e.g.,][]{Hay2000econometrics}. The test statistic is obtained by carrying out the unrestricted estimation and the estimation under the null hypothesis, computing the desired test statistic for both estimations, and then taking the difference of the two test statistics. This difference is asymptotically $\chi^2$-distributed with $T_{J_{H_1}} - T_{J_{H_0}}$ degrees of freedom, where $T_{J_{H_1}}$ are the degrees of freedom of the unrestricted model and $T_{J_{H_0}}$ those of the restricted one \citep[see][]{Hay2000econometrics}.
%\citep[see][p.218-221 and 227-228]{Hay2000econometrics}.










\subsection{Testing linear hypotheses} \label{sec:linHyp}
The Wald test is one possibility to test general linear hypotheses of the form
\begin{align*}
H_0: \boldsymbol{R} \boldsymbol{\theta} = \boldsymbol{r},
\end{align*}
where the matrix $\boldsymbol{R}$ is a $c \times p$ matrix, which selects the elements of the $p \times 1$ vector of population parameters $\boldsymbol{\theta}$ required to express the left-hand side of the $c$ equations of the null hypothesis (i.e., the restrictions under the null) and the vector $\boldsymbol{r}$ is a $c \times 1$ vector that states the right-hand side of the equations.
Tests of three different standard null hypotheses are currently available in \pkg{pdynmc}: (a) all population parameters corresponding to the right-hand side variables of the linear dynamic panel data model are zero jointly, (b) all population parameters corresponding to the lagged-dependent and non-lagged dependent explanatory variables are zero jointly, and (c) all population parameters corresponding to the time dummies are zero jointly.

In case of one-step GMM estimation, the Wald statistic can be obtained from
\begin{align*}
T_W = n \cdot (\boldsymbol{R} \hat{\boldsymbol{\theta}}_{1} - \boldsymbol{r})' \left( \boldsymbol{R} \ \widehat{\boldsymbol{\Omega}}(\hat{\boldsymbol{\theta}}_{1}) \ \boldsymbol{R}' \right)^{-1} (\boldsymbol{R} \hat{\boldsymbol{\theta}}_{1} - \boldsymbol{r}), \quad \text{with} \quad T_W \overset{a}{\sim} \chi^2(c).
\end{align*}
In order to calculate the Wald statistic for two-step GMM estimation, the vector of parameter estimates $\hat{\boldsymbol{\theta}}_{1}$ and the estimated variance covariance matrix of the parameter estimates $\widehat{\boldsymbol{\Omega}}(\hat{\boldsymbol{\theta}}_{1})$ need to be replaced by their equivalents from two-step estimation.
Under suitable conditions\footnote{See \citet{NewMcF1994,Hay2000econometrics} and the discussion in \citet{Fri2019}.}, the Wald statistic $T_W$ is asymptotically $\chi^2$-distributed with $c$ degrees of freedom \citep[see][]{Hay2000econometrics}. The estimated variance covariance matrix of coefficient estimates to be used in both calculations may be either the non-robust versions stated in Equations (\ref{EQ14:VCov_beta_1step}) and (\ref{EQ16:VCov_beta_2step}) or the robust/corrected versions of the matrix from Equations (\ref{EQ15:VCov_beta_1step_rob}) and (\ref{EQ17:WindmeijerCorrSE}). The equivalent matrices need to be chosen in multiple-step GMM estimation to obtain the corresponding Wald statistic.
As usual, a large value of the Wald statistic casts doubt on the null hypothesis.
%\citep[see][p.211]{Hay2000econometrics}























%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section[Related software and R-packages]{Related software and \proglang{R}-packages} \label{sec:related software}
GMM estimation of linear dynamic panel data models based on linear moment conditions is available in a number of software environments and packages such as \proglang{Gauss}, \proglang{Ox}, \proglang{R}, and \proglang{Stata}. We highlight the particularities of a few selected implementations here.
%Additionally, we summarize the functionality of \pkg{pdynmc} and elaborate on some differences to existing software.


The \proglang{Gauss} and \proglang{Ox} implementations, which are both named \pkg{DPD} \citep{AreBon1988DPD,DooAreBon2012dpd}, represent an important reference for later software. The packages include the computation of one-step and two-step closed form GMM estimators and standard specification testing such as overidentifying restrictions tests, serial correlation tests, and Wald tests. Some estimators for static panel data models like the within estimator and feasible generalized least squares estimation are also available.


In \proglang{Stata} \citep[e.g.,][]{Sta2011}, the command \pkg{xtabond2} \citep{Roo2009StJ} is a popular choice for GMM estimation of linear dynamic panel data models based on linear moment conditions. The command calculates the closed form solution for the estimators and is accompanied by extended model diagnostics to assess the validity of certain subsets of moment conditions and the overall specification. Employing nonlinear moment conditions and GMMCU are not supported.
The recently contributed command \pkg{xtdpdgmm} \citep{Kri2019} enables the user to include nonlinear moment conditions into the analysis. The command also allows for GMMCU and the numerical optimization of the GMM objective function is based on a Gauss-Newton technique.


In \proglang{R} \citep{RCo2019}, the packages \pkg{plm} \citep{CroMil2008} and \pkg{panelvar} \citep{SigFer2018panelvar} implement the functionality available in \pkg{xtabond2} with some additional features. For example, the package \pkg{panelvar} allows the user to perform lag selection based on information criteria, structural analysis based on impulse response functions, the computation of corresponding bootstrapped confidence intervals, and allows for GMMCU. The package \pkg{plm} provides a variety of functions for the estimation of static and linear panel models such as the within estimator, different random effects estimators, feasible generalized least squares estimation, and a number of different specification tests. The function \pkg{pgmm} is specifically designed to estimate linear dynamic panel data models -- GMMCU is not implemented. Both \proglang{R} packages do not allow to incorporate nonlinear moment conditions into the analysis.




%Currently, \pkg{pdynmc} provides:
%\begin{itemize}
%\item Automatic handling of unbalanced panel data sets in the estimation procedure.
%\item One-step, two-step and continuously updating GMM estimation based on the linear HNR- and AB moment conditions and the nonlinear AS moment conditions. {\color{red}{[M:] GMMCU still requires minor texttt adjustments and -- most importantly -- speeding up the texttt}}
%\item Including general lag structures for the (lagged dependent and all further) explanatory variables. {\color{red}{[M:] Beyond two lags this still requires some adjustments, as the increase in data requirements is not implemented fully flexible, yet.}}
%\item Other endogenous, predetermined or exogenous explanatory variables besides the lagged dependent variable can be included in estimation. From these explanatory variables, the moment conditions outlined in Section \ref{sec:ldpdm} are then derived. Additionally, further controls and time dummies can be included, which instrument themselves.
%\item Time dummies can be specified implicitly by denoting the variables from which to create the dummies or explicitly (if a particular set of time dummies should be included in the estimation) by giving the names of the respective dummies. An alternative to handle time dummies is to partial out the time effects (see, e.g., \citep{Gil1984}). This is also available as an option.
%\item As an alternative to the internal or natural instruments typically employed in GMM estimation (and obtained by lagging the dependent variable), endogenous variables can also be instrumented by external instruments (if available). {\color{red}{[M:] This requires some further checks and was primarily meant to ensure that Browning and Collado (2007) can be estimated. Might not be required anymore -- but could be a further selling point??}}
%\item The standard options for the weighting matrix outlined in Section \ref{section:1_2_step}.
%\item For the coefficient estimates, three types of standard errors are currently available: for both, one-step and two-step standard errors unadjusted and robust/corrected standard errors are available; alternatively, one-step standard errors can be reported for two-step GMM estimation.
%\item The different optimization techniques implemented in the \proglang{R}-package \pkg{optimr} (\citep{NasVar2016optimr}) to obtain coefficient estimates.
%\item The sparse structure of the matrices is accounted for by using the \proglang{R}-package \pkg{Matrix} \citep{BatMae2016}, which speeds up estimation. {\color{red}{[M:] Still to come: use package \texttt{data.table}, as this should speed up the estimation process further.}}
%\end{itemize}




















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section[Sample session]{Sample session} \label{sec:example}
The functionality of \pkg{pdynmc}
%\footnote{All results contained in this section are reproducible with the functions provided in the accompanying \proglang{R}-code. The estimation function is approximately 2,000 lines of code and provides a flexible implementation of the estimation of linear dynamic panel data models with linear and nonlinear moment conditions. Most of the options described in the text are already fully implemented. Additionally, functions to carry out the tests in Section \ref{sec:covariance_matrix} are also provided.}
is illustrated by replicating some of the empirical results in \citet{AreBon1991}. Additionally, we show how to incorporate the linear AB and the nonlinear AS moment conditions into the analysis. We explain all arguments which need to be set to reproduce the results and point out some alternative options. We also draw comparisons between \pkg{pdynmc}, the \proglang{Stata} implementations \pkg{xtabond2}, \pkg{xtdpdgmm}, and the \pkg{pgmm} function in the \proglang{R}-package \pkg{plm} -- where we are aware of differences between the implementations.

The data set employed in \citet{AreBon1991} is an unbalanced panel of $n=140$ firms located in the UK observed over $T=9$ time periods and is available from package \proglang{R}-\pkg{plm}:
\begin{verbatim}
data(EmplUK, package = "plm")
dat                  <- EmplUK
dat[,c(4:7)]         <- log(dat[,c(4:7)])
names(dat[,c(4:7)])  <- c("n", "w", "k", "ys")
\end{verbatim}

The authors investigate employment equations and consider the dynamic specification
\begin{align} \label{EQ18:ABEstimation}
n_{i,t} =& \alpha_1 n_{i,t-1} + \alpha_2 n_{i,t-2} + \\
&\beta_1 w_{i,t} + \beta_2 w_{i,t-1} + \beta_3 k_{i,t} + \beta_4 k_{i,t-1} + \beta_5 k_{i,t-2} + \beta_6 ys_{i,t} + \beta_7 ys_{i,t-1} + \beta_8 ys_{i,t-2} + \nonumber \\
& \gamma_3 d_3 + \dots + \gamma_T d_T + \eta_i + \varepsilon_{i,t}, \qquad i = 1,...,n;\ t = 3,...,T. \nonumber
\end{align}
In the equation, $i$ denotes the firm and $t$ is the time series dimension. The natural logarithm of employment $n$ is explained by its first two lags and the further explanatory variables natural logarithm of wage $w$, natural logarithm of capital $k$, natural logarithm of output $ys$, and their lags of order up to one (for $w$) or two (for $k$ and $ys$). The variables $d_3, \dots, d_T$ are time dummies with corresponding coefficients $\gamma_3, \dots, \gamma_T$; the unobserved individual-specific effect is represented by $\eta$, and $\varepsilon$ is an idiosyncratic remainder component. The goal is to estimate the lag parameters $\alpha_1$ and $\alpha_2$ and the coefficients of the further explanatory variables $\beta_j$, with $j=1, \dots, 8$, while controlling for (unobserved) time effects and accounting for unobserved individual-specific heterogeneity.










\subsection{GMM estimation with HNR moment conditions} \label{sec:HNRmcEstimation}
When reproducing the results in Table 4 on p.290 of \citet{AreBon1991} with \pkg{pdynmc}, the model structure of the underlying Equation (\ref{EQ18:ABEstimation}) can be specified by:
\begin{verbatim}
m1 <- pdynmc(
  dat = dat, varname.i = "firm", varname.t = "year",
  use.mc.diff = TRUE, use.mc.lev = FALSE, use.mc.nonlin = FALSE,
  include.y = TRUE, varname.y = "emp", lagTerms.y = 2,
  fur.con = TRUE, fur.con.diff = TRUE, fur.con.lev = FALSE,
  varname.reg.fur = c("wage", "capital", "output"),
  lagTerms.reg.fur = c(1,2,2),
  include.dum = TRUE, dum.diff = TRUE, dum.lev = FALSE, varname.dum = "year",
  w.mat = "iid.err", std.err = "corrected",
  estimation = "onestep", opt.meth = "none")
\end{verbatim}
The first arguments relate to the data set (\code{dat}), the individual (\code{varname.i}), and time series dimension (\code{varname.t}).
Next, the moment conditions are defined -- here, only moment conditions from equations in differences are used.
The moment conditions are derived for the dependent variable (\code{varname.y}), and for the corresponding number of lags of the dependent variable to be included as explanatory variables (\code{lagTerms.y}).
Further non-lagged dependent explanatory variables (\code{fur.con}) are included in the equations in differences, not in the equations in levels (compare \code{fur.con.diff = TRUE, fur.con.lev = FALSE}), their variable names are stated in (\code{varname.reg.fur}), their lag structure is specified in (\code{lagTerms.reg.fur}).
Note that the first element of the vector denoting the lag structure corresponds to the first element of the vector with the variable names, the second element to the second, and so on.
Also note that all names given in the vectors that refer to variables in the data set need to have the same names as in the data set.
The time dummies are included by (\code{include.dum}) into the equations in differences not in the equations in levels (\code{dum.diff = TRUE, dum.lev = FALSE}), the dummy indicator variable is (\code{varname.dum}). Note that time dummies can be constructed from one or multiple variables by \pkg{pdynmc} by simply passing a scalar or vector with the respective variable names in the data to \code{varname.dum}.
Specifying the matrix $\boldsymbol{H}$ in Equation (\ref{EQ10:W1step}), which governs the structure of the one-step weighting matrix, and carrying out one-step estimation can be achieved by the commands in the last two codelines. Choosing the option \code{iid.err} uses the matrix $\boldsymbol{H}_{HNR}$ proposed by \citet{AreBon1991}. Alternatively, an identity matrix can be employed for $\boldsymbol{H}$ by the option \code{identity}.
Code \code{std.err = "corrected"} yields robust standard errors for one-step estimation (in case of a two-step estimation, the correction of \citep{Win2005} was employed by this argument).

One-step, two-step, and GMMCU (accessible by setting the argument \code{estimation} to \code{cue}) estimation in \pkg{pdynmc} is carried out by numerical optimization of the GMM objective function given in Equation (\ref{EQ09:Objective-function-GMM}). Since a closed form solution exists for the estimator when employing only linear moment conditions, numerical optimization is not required and can be switched off by setting \code{opt.meth = "none"}.

The standard output can be accessed via \code{summary(m1)} and yields
%\FloatBarrier
\begin{table}[hptb]
\centering
\begin{scriptsize}
\caption{Column (a1) of Table 4 in \citet{AreBon1991}} \label{Tab01:HNR1step}
\begin{tabular}{lcccc}
  \hline
 & \multicolumn{1}{c}{{Estimate}} & \multicolumn{1}{c}{{Std.Err.rob}} & \multicolumn{1}{c}{{z.rob}} & \multicolumn{1}{c}{{Pr($>$$|$z.rob$|$)}}\tabularnewline
  \hline
  L1.n  & 0.68623{***}    & 0.14459 & 4.74600 & $<0.001$ \tabularnewline
  L2.n  & -0.08536                  & 0.05602 & -1.52400 & 0.12751 \tabularnewline
  w     & -0.60782{***}   & 0.17821 & -3.41100 & $<0.001$ \tabularnewline
  L1.w  & 0.39262{*}          & 0.16799 & 2.33700 & 0.01944 \tabularnewline
  k     & 0.35685{***}    & 0.05902 & 6.04600 & $<0.001$ \tabularnewline
  L1.k  & -0.05800                  & 0.07318 & -0.79300 & 0.42778 \tabularnewline
  L2.k  & -0.01995                  & 0.03271 & -0.61000 & 0.54186 \tabularnewline
  ys    & 0.60851{***}    & 0.17253 & 3.52700 & $<0.001$ \tabularnewline
  L1.ys & -0.71116{**}      & 0.23172 & -3.06900 & 0.00215 \tabularnewline
  L2.ys & 0.10580                   & 0.14120 & 0.74900 & 0.45386 \tabularnewline
  1979  & 0.00955                   & 0.01029 & 0.92900 & 0.35289 \tabularnewline
  1980  & 0.02202                   & 0.01771 & 1.24300 & 0.21387 \tabularnewline
  1981  & -0.01177                  & 0.02951 & -0.39900 & 0.68989 \tabularnewline
  1982  & -0.02706                  & 0.02928 & -0.92400 & 0.35549 \tabularnewline
  1983  & -0.02132                  & 0.03046 & -0.70000 & 0.48393 \tabularnewline
  1984  & -0.00770                  & 0.03141 & -0.24500 & 0.80646 \tabularnewline
   \hline
   \hline
    \multicolumn{5}{l}{{\scriptsize{}Equations in first differences: $L\left(2/8\right).n,D.w,L.D.w,D.k,$}}\tabularnewline
    \multicolumn{5}{l}{{\scriptsize{}$L.D.k,L2.D.k,D.ys,L.D.ys,L2.D.ys,D.1979-D.1984$}}\tabularnewline
   \multicolumn{5}{l}{{\scriptsize{}{*} $p<0.05$, {**} $p<0.01$, {***}
$p<0.001$ (refers to $t$-test of the null}}\tabularnewline
   \multicolumn{5}{l}{{\scriptsize{} that the coefficient is equal to zero)}}\tabularnewline
\end{tabular}
\end{scriptsize}
\end{table}
%\FloatBarrier
the estimation results when specifying all arguments as stated in this section and reproduces the results in Table 4, column (a1) on p.290 of \citet{AreBon1991}.

Changing the argument \code{estimation} to \code{twostep} yields the two-step GMM coefficient estimates (the \code{pdynmc}-output object is assigned to \code{m2}) from Table 4, column (a2) on p.290 of \citet{AreBon1991}.
%\FloatBarrier
\begin{table}[htbp]
\centering
\begin{scriptsize}
\caption{Column (a2) of Table 4 in \citet{AreBon1991}} \label{Tab02:HNR2step}
\begin{tabular}{lllll}
  \hline
 & \multicolumn{1}{c}{{Estimate}} & \multicolumn{1}{c}{{Std.Err.rob}} & \multicolumn{1}{c}{{z.rob}} & \multicolumn{1}{c}{{Pr($>$$|$z.rob$|$)}}\tabularnewline
  \hline
  L1.n  & 0.62871{**}   & 0.19341 & 3.25100 & 0.00115 \tabularnewline
  L2.n  & -0.06519      & 0.04505 & -1.44700 & 0.14790 \tabularnewline
  w     & -0.52576{***} & 0.15461 & -3.40100 & $<0.001$ \tabularnewline
  L1.w  & 0.31129       & 0.20300 & 1.53300 & 0.12528 \tabularnewline
  k     & 0.27836{***}  & 0.07280 & 3.82400 & $<0.001$ \tabularnewline
  L1.k  & 0.01410       & 0.09246 & 0.15200 & 0.87919 \tabularnewline
  L2.k  & -0.04025      & 0.04327 & -0.93000 & 0.35237 \tabularnewline
  ys    & 0.59192{***}  & 0.17309 & 3.42000 & $<0.001$ \tabularnewline
  L1.ys & -0.56599{*}   & 0.26110 & -2.16800 & 0.03016 \tabularnewline
  L2.ys & 0.10054       & 0.16110 & 0.62400 & 0.53263 \tabularnewline
  1979  & 0.01122       & 0.01168 & 0.96000 & 0.33706 \tabularnewline
  1980  & 0.02307       & 0.02006 & 1.15000 & 0.25014 \tabularnewline
  1981  & -0.02136      & 0.03324 & -0.64200 & 0.52087 \tabularnewline
  1982  & -0.03112      & 0.03397 & -0.91600 & 0.35967 \tabularnewline
  1983  & -0.01799      & 0.03693 & -0.48700 & 0.62626 \tabularnewline
  1984  & -0.02337      & 0.03661 & -0.63800 & 0.52347 \tabularnewline
   \hline
   \hline
   \multicolumn{5}{l}{{\scriptsize{}Equations in first differences: $L\left(2/8\right).n,D.w,L.D.w,D.k,$}}\tabularnewline
    \multicolumn{5}{l}{{\scriptsize{}$L.D.k,L2.D.k,D.ys,L.D.ys,L2.D.ys,D.1979-D.1984$}}\tabularnewline
   \multicolumn{5}{l}{{\scriptsize{}{*} $p<0.05$, {**} $p<0.01$, {***}
$p<0.001$ (refers to $t$-test of the null}}\tabularnewline
   \multicolumn{5}{l}{{\scriptsize{} that the coefficient is equal to zero)}}\tabularnewline
\end{tabular}
\end{scriptsize}
\end{table}
%\FloatBarrier
Note that the standard errors presented in Table \ref{Tab02:HNR2step} are based on the Windmeijer-correction and deviate from the conventional standard errors reported in \citet{AreBon1991}. The standard errors from the original analysis can be reproduced by setting \code{std.err = "unadjusted"}.
%Alternatively, this option can be set to \code{onestep}, which reports one-step standard errors for the two-step coefficient estimates. Computing robust (for GMM1s) or Windmeijer-corrected (for GMM2s) standard errors requires setting \code{std.err} to \code{corrected} (the default in \pkg{pdynmc}).

Regarding the arguments
\begin{verbatim}
  use.mc.diff = TRUE, include.y = TRUE, include.x = FALSE
\end{verbatim}
it has to be noted that the HNR moment conditions (\code{use.mc.diff}) derived from the lagged dependent variable (\code{include.y}) are employed, while none are derived from the further non-lagged dependent explanatory variables (by default \code{include.x = FALSE}). The latter argument implies that the non-lagged dependent explanatory variables in the model are assumed to be exogenous and instrument themselves.

Different capabilities for testing hypotheses about the population parameters are available in \pkg{pdynmc}. Among them are the tests for serial correlation in the idiosyncratic remainder components proposed by \citet{AreBon1991},
%Sargan tests,
Hansen tests, and Wald tests. In the following, carrying out these tests and interpreting the results is briefly illustrated based on the two-step GMM estimation results presented in Table \ref{Tab02:HNR2step}.

Employing the test for second order serial correlation of \citet{AreBon1991} described in Section \ref{sec:specTesting} by \code{m.test(m2, t.order = 2)} yields:
\begin{verbatim}
        Serial correlation test of degree 2

data:  GMM Estimation; H0: no serial correlation of order 2 in epsilon
normal = -0.36744, p-value = 0.7133
\end{verbatim}
The test does not reject the null hypothesis at any plausible significance level and does not provide any indication that the model specification might be inadequate. The test statistic and $p$-value are similar to \pkg{xtabond2} and \pkg{pgmm}.

Computing the Hansen $J$-test of the overidentifying restrictions described in Section \ref{sec:oirTesting} by \code{j.test(m2)} yields:
\begin{verbatim}
        J-Test of Hansen

data:  GMM Estimation; H0: overidentifying restrictions valid
chisq = 31.381, df = 25, p-value = 0.1767
\end{verbatim}
The test does not reject the overidentifying restrictions and does not provide any indications that the validity of the instruments employed in estimation may be in doubt. Comparing the results to \pkg{xtabond2} shows that the degrees of freedom and the $p$-value differ. We consider 25 degrees of freedom to be the appropriate number here, as 41 instruments are employed in estimation to obtain 16 coefficient estimates. It seems that the function \pkg{xtabond2} does not correct the degrees of freedom for the number of dummies dropped in estimation\footnote{Dummies are dropped by the estimation routine in case of high collinearity.}. The difference in the $p$-value is due to the differences in the degrees of freedom. Our results are equivalent to the results of \pkg{pgmm} for the overidentifying restrictions test. In \pkg{pgmm}, the above test is referred to as `Sargan test'.

%For the Sargan test (\code{sargan.fct(m2}) we get:
%\begin{verbatim}
%        Sargan Test
%
%data:  GMM Estimation; H0: overidentifying restrictions valid
%chisq = 54.756, df = 25, p-value = 0.0005297
%\end{verbatim}
%
%Contrary to the $J$-Test of Hansen, the Sargan test rejects the null hypothesis and raises doubts about the instrument set. The results of both tests should, however, be interpreted with caution, as the Sargan test statistic is inconsistent when heteroscedasticity is present and the power of the $J$-Test is weakened by the presence of many instruments \citep[see][]{Roo2009StJ}.
%Comparing the result to \pkg{xtabond2} reveals differences besides the degrees of freedom and the corresponding $p$-value: The test statistics are not identical. The differences stem from the calculation requiring a correction of the degrees of freedom. While we correct the number of observations available in estimation for missing values in the case of an unbalanced panel data set, \pkg{xtabond2} does not. Additionally, \pkg{xtabond2} seems to use the number of instruments employed in estimation to correct the degrees of freedom without adjusting for the time dummies dropped in estimation, while we make this adjustment.

For the Wald test illustrated in Section \ref{sec:linHyp}, consider the null hypothesis that the population parameters of all coefficients included in the model are zero jointly (\code{wald.fct(param = "all", object = m2}):
\begin{verbatim}
        Wald test

data:  GMM Estimation; H0: beta = 0; tested model parameters: all
chisq = 1104.7, df = 16, p-value < 2.2e-16
\end{verbatim}
The test rejects the null hypothesis. Comparing the test result to the implementation of the test in \pkg{xtabond2} -- again -- reveals differences concerning the degrees of freedom. We consider 16 to be the appropriate number of degrees of freedom here, since this corresponds to the number of estimated parameters. As noted previously, the differences seem to stem from \pkg{xtabond2} not adjusting the degrees of freedom for the dummies dropped in estimation. Alternative hypotheses that can be tested via the Wald test in \pkg{pdynmc} are that all slope parameters are zero jointly and that all parameters corresponding to the time dummies are zero jointly (\code{param = "time.dum"} only tests the time dummies, while \code{param = "slope"} only tests the slope parameters).












\subsection{GMM estimation with HNR and AB moment conditions} \label{sec:HNRABmcEstimation}
When the `constant correlated effects' assumption stated in Equation (\ref{EQ07:CCE}) holds, the HNR moment conditions from equations in differences employed in Section \ref{sec:HNRmcEstimation} can be extended by the AB moment conditions from equations in levels.
The AB moment conditions are particularly useful for data generating processes, which are highly persistent \citep{BluBon1998}. In this case, identification by the HNR moment conditions from equations in levels may fail and GMM estimation based on HNR moment conditions is documented to possess poor finite sample performance \citep[see, e.g.,][]{BluBon1998,BluBonWin2001,BunSar2015}.

In \pkg{pdynmc}, the AB moment conditions from equations in levels can be (additionally) incorporated by :
\begin{verbatim}
use.mc.lev = TRUE
\end{verbatim}
In principle, both, the time dummies and the further explanatory variables can be included in the equations in first differences and the level equations. It is recommended, though, to include the dummies only in one of the equations, as it can be shown that incorporating them in both equations renders one set of dummies redundant for estimation -- while for the non-lagged dependent explanatory variables, this equivalence does not hold.\footnote{Note that this is the case in balanced panels. The results may also not be numerically identical across function calls for different choices of the one-step weighting matrix. For a discussion, see https://www.statalist.org/forums/forum/general-stata-discussion/general/1357268-system-gmm-time-dummies.} The arguments that govern accommodating non-lagged dependent explanatory variables and time dummies which instrument themselves in the levels equations are:
\begin{verbatim}
fur.con.lev = TRUE, dum.lev = TRUE
\end{verbatim}
Using these arguments together with the earlier specified ones -- except for setting `dum.diff = FALSE' -- leads to the time dummies being included in the level equations and the further explanatory variables being included in both equations.


In order to obtain coefficient estimates, a decision about the matrix $\boldsymbol{H}$ in the one-step weighting matrix is required. When using the HNR and AB moment conditions, the decision about $\boldsymbol{H}$ effectively involves specifying the matrices $\boldsymbol{A}$, $\boldsymbol{B}$, and $\boldsymbol{C}$ in the general structure given in Section \ref{section:1_2_step}. As mentioned, the diagonal elements $\boldsymbol{A}$ and $\boldsymbol{C}$ reflect the expected variance covariance properties within a set of moment conditions, while $\boldsymbol{B}$ reflects the expected covariances across different sets of moment conditions. In the given setting, $\boldsymbol{A}$ corresponds to the variance covariance properties of the HNR moment conditions, $\boldsymbol{C}$ to those of the AB moment conditions, and $\boldsymbol{B}$ to those across the HNR and AB moment conditions. Three different options are currently available in \pkg{pdynmc} to set up the weighting matrix \code{w.mat}: \code{iid.err}, \code{identity}, and \code{zero.cov}. The first option leads to $\boldsymbol{H}_{HNR}$ being used for $\boldsymbol{A}$, an identity for $\boldsymbol{C}$, and a matrix $\boldsymbol{B}$, such that $\boldsymbol{B} \boldsymbol{B}' = \boldsymbol{H}_{HNR}$. Setting \code{w.mat} to \code{identity} leads to an identity matrix being used for the diagonal matrices $\boldsymbol{A}$ and $\boldsymbol{C}$ and an adequately dimensioned matrix $\boldsymbol{B}$ with 1 on the diagonal\footnote{Note that the matrix $\boldsymbol{B}$ is not necessarily a quadratic matrix.}.
When using the option \code{zero.cov}, the matrices $\boldsymbol{A}$ and $\boldsymbol{C}$ are as for option \code{iid.err}, but $\boldsymbol{B}$ is set to a null matrix. In case nonlinear moment conditions are used, the part of $\boldsymbol{H}$ which corresponds to the nonlinear moment conditions is set to an identity for all choices of \code{w.mat}. All elements of the matrices containing the expected covariance properties of the nonlinear moment conditions with other moment conditions are always set to zero.

%\FloatBarrier
\begin{table}[hptb]
\centering
\begin{scriptsize}
\caption{\citet{AreBon1991} estimation with AB moment conditions} \label{Tab03:HNRAB2step}
\begin{tabular}{lllll}
  \hline
 & \multicolumn{1}{c}{{Estimate}} & \multicolumn{1}{c}{{Std.Err.corr}} & \multicolumn{1}{c}{{z.corr}} & \multicolumn{1}{c}{{Pr($>$$|$z.corr$|$)}}\tabularnewline
  \hline
L1.n    & 1.11650{***}     & 0.05192 & 21.50500 & $<0.001$ \tabularnewline
  L2.n  & -0.11352{*}      & 0.04764 & -2.38300 & 0.01717 \tabularnewline
  w     & -0.44169{**}     & 0.15175 & -2.91100 & 0.00360 \tabularnewline
  L1.w  & 0.42159{**}      & 0.15528 & 2.71500 & 0.00663 \tabularnewline
  k     & 0.28618{***}     & 0.04751 & 6.02400 & $<0.001$ \tabularnewline
  L1.k  & -0.16474{*}      & 0.06589 & -2.50000 & 0.01242 \tabularnewline
  L2.k  & -0.12321{**}     & 0.04250 & -2.89900 & 0.00374 \tabularnewline
  ys    & 0.55793{**}      & 0.17651 & 3.16100 & 0.00157 \tabularnewline
  L1.ys & -0.67392{**}     & 0.21707 & -3.10500 & 0.00190 \tabularnewline
  L2.ys &  0.13372         & 0.14344 & 0.93200 & 0.35134 \tabularnewline
  1978  & -0.05313         & 0.35746 & -0.14900 & 0.88155 \tabularnewline
  1979  & -0.03697         & 0.35698 & -0.10400 & 0.91717 \tabularnewline
  1980  & -0.01933         & 0.35429 & -0.05500 & 0.95614 \tabularnewline
  1981  & -0.05791         & 0.34696 & -0.16700 & 0.86737 \tabularnewline
  1982  & -0.04334         & 0.34512 & -0.12600 & 0.89973 \tabularnewline
  1983  & -0.01818         & 0.34583 & -0.05300 & 0.95773 \tabularnewline
  1984  & -0.02815         & 0.34914 & -0.08100 & 0.93544 \tabularnewline
   \hline
   \hline
   \multicolumn{5}{l}{{\scriptsize{}Equations in first differences: $L\left(2/8\right).n,D.w,L.D.w,L2.D.w,$}}\tabularnewline
    \multicolumn{5}{l}{{\scriptsize{}$D.k,L.D.k,L2.D.k,D.ys,L.D.ys,L2.D.ys$}}\tabularnewline
    \multicolumn{5}{l}{{\scriptsize{}Equations in levels: $L\left(1/7\right).D.n,w,L.w,L2.w,k,L.k,L2.k,$}}\tabularnewline
    \multicolumn{5}{l}{{\scriptsize{}$ys,L.ys,L2.ys,1978-1984$}}\tabularnewline
   \multicolumn{5}{l}{{\scriptsize{}{*} $p<0.05$, {**} $p<0.01$, {***}
$p<0.001$ (refers to $t$-test of the null}}\tabularnewline
   \multicolumn{5}{l}{{\scriptsize{} that the coefficient is equal to zero)}}\tabularnewline
\end{tabular}
\end{scriptsize}
\end{table}
%\FloatBarrier

The results presented in Table \ref{Tab03:HNRAB2step} are the two-step estimates of column (a2) of Table 4 in \citet{AreBon1991} extended by the AB moment conditions. All arguments are specified as described above. Including the AB moment conditions into the analysis leads to substantial changes in the coefficient estimates of the first lag of the dependent variable. Note that the results indicate a markedly higher persistence of employment and render including two lags of the dependent variable questionable \citep[][e.g., estimate a version of the equation which contains only one lag of all explanatory variables]{BluBon1998}. Note that the coefficient estimates of the explanatory variables, besides the first lag of the dependent variable, appear to be similar across estimations.


Equivalent results to Table \ref{Tab03:HNRAB2step} can be obtained from the \pkg{pgmm} function in the \pkg{plm}-package -- besides some minor numerical differences at the fifth digit. When replicating the results with \pkg{xtabond2}, differences in the implementations become obvious: The instrument set for the AB moment conditions is extended in similar fashion to the HNR moment conditions in \pkg{xtabond2}, while this is not the case in \pkg{pgmm}. An argument is available in \pkg{pdynmc} to extend the instrument set as in \pkg{xtabond2}:
\begin{verbatim}
inst.stata = TRUE
\end{verbatim}
Due to the reasons described in Section \ref{sec:ExtAssumpt}, this argument is set to \code{FALSE} per default. When setting the option to \code{TRUE}, the results from \pkg{xtabond2} and \pkg{pdynmc} are very close to our results.
%There are also differences between the results from \pkg{xtabond2} and \pkg{pdynmc} which seem to stem from both functions using a slightly different set of instruments (the instrument count differs by one).
% -- but not identical.
%The reason for the differences seems to be the instrument set, as \pkg{xtabond2} reports a lower instrument count. When contrasting the instrument matrices used by \pkg{xtabond2} and \pkg{pdynmc}, though, it appears that both functions employ the exact same instruments. It is currently unclear to us from where the difference in the results emerges.







\subsection{GMM estimation with HNR and AS moment conditions}
Recall, that the linear AB moment conditions from equations in levels comprise the nonlinear AS moment conditions and render them redundant for estimation (\citealp[][]{BluBon1998}; \citealp[a derivation is provided in][]{Fri2019}). Both sets of moment conditions may be useful in GMM estimation when the lag parameter is close to unity and it can be shown that extending the HNR moment conditions by either the AB- or the AS moment conditions may identify the lag parameter -- even when the individual moment conditions fail to do so \citep{BunKle2014,GorHanXue2016}. The AB moment conditions require the `constant correlated effects' assumption, while the AS moment conditions only require standard assumptions to hold. Therefore, the latter may be useful in situations where the `constant correlated effects' assumption is in doubt and the statistician aims to investigate a highly persistent dynamic process with a structure similar to Equation (\ref{EQ01:lin-dyn-pdm}).
In \pkg{pdynmc}, including nonlinear moment conditions into the analysis is available via:
\begin{verbatim}
use.mc.nonlin = TRUE
\end{verbatim}

%\FloatBarrier
\begin{table}[hptb]
\centering
\begin{scriptsize}
\caption{Estimation in \citet{AreBon1991} extended by AS moment conditions} \label{Tab04:HNRAS2step}
\begin{tabular}{lllll}
  \hline
 & Estimate & Std.Err.corr & z.corr & Pr($>$$|$z.corr$|$) \\
  \hline
L1.n    & 1.10900{***}  & 0.06624 & 16.74100  & $<0.001$ \\
  L2.n  & -0.06960      & 0.06290 & -1.10700  & 0.26829 \\
  L0.w  & -0.43855{**}  & 0.14709 & -2.98100  & 0.00287 \\
  L1.w  & 0.43632{**}   & 0.14695 & 2.96900   & 0.00299 \\
  L0.k  & 0.30913{***}  & 0.05093 & 6.07000   & $<0.001$ \\
  L1.k  & -0.19133{**}  & 0.06997 & -2.73400  & 0.00626 \\
  L2.k  & -0.14991{**}  & 0.04806 & -3.11900  & 0.00181 \\
  L0.ys & 0.63172{***}  & 0.17203 & 3.67200   & $<0.001$ \\
  L1.ys & -0.67326{**}  & 0.21243 & -3.16900  & 0.00153 \\
  L2.ys & 0.05287       & 0.14523 & 0.36400   & 0.71586 \\
  1978  & -0.13575      & 0.38199 & -0.35500  & 0.72259 \\
  1979  & -0.11641      & 0.38167 & -0.30500  & 0.76037 \\
  1980  & -0.09147      & 0.37873 & -0.24200  & 0.80878 \\
  1981  & -0.12311      & 0.37084 & -0.33200  & 0.73989 \\
  1982  & -0.11354      & 0.36750 & -0.30900  & 0.75732 \\
  1983  & -0.09785      & 0.36719 & -0.26600  & 0.79024 \\
  1984  & -0.10241      & 0.37359 & -0.27400  & 0.78408 \\
   \hline
      \hline
   \multicolumn{5}{l}{{\scriptsize{}Equations in first differences: $L\left(2/8\right).n, u ,D.w,L.D.w,L2.D.w,$}}\tabularnewline
    \multicolumn{5}{l}{{\scriptsize{}$D.k,L.D.k,L2.D.k,D.ys,L.D.ys,L2.D.ys$}}\tabularnewline
    \multicolumn{5}{l}{{\scriptsize{}Equations in levels: $w,L.w,L2.w,k,L.k,L2.k,ys,L.ys,L2.ys,$}}\tabularnewline
    \multicolumn{5}{l}{{\scriptsize{}$1978-1984$}}\tabularnewline
   \multicolumn{5}{l}{{\scriptsize{}{*} $p<0.05$, {**} $p<0.01$, {***}
$p<0.001$ (refers to $t$-test of the null}}\tabularnewline
   \multicolumn{5}{l}{{\scriptsize{} that the coefficient is equal to zero)}}\tabularnewline
\end{tabular}
\end{scriptsize}
\end{table}
%\FloatBarrier

When extending the analysis of \citet{AreBon1991} by the nonlinear AS moment conditions, the results differ substantially from Table \ref{Tab02:HNR2step} and are very similar to the coefficient estimates shown in Table \ref{Tab03:HNRAB2step}. This casts doubt on the HNR moment conditions and may be a hint that there is high persistence in the employment process --  as high persistence leads to the lag parameters not being identified by the HNR moment conditions \citep{BunKle2014,GorHanXue2016}.
Note that since the unobservable error term $u$ (which can be expressed in terms of observable model components and parameters) is included in the instrument set for the equations in first differences, nonlinear moment conditions are employed in estimation. Also note that the coefficient estimates in Table \ref{Tab04:HNRAS2step} are very close to coefficient estimates obtained from \pkg{xtdpdgmm}.






\subsection{Iterated GMM}
Iterated GMM can be used, by specifying the following commands:
\begin{verbatim}
estimation = "iterative", max.iter = NULL, iter.tol = NULL,
\end{verbatim}
When \code{estimation = "iterative"} is used, \code{max.iter} specifies the maximum number of iterations, \code{iter.tol} the search tolerance w.r.t.\ convergence. Unless changed by the user, \code{max.iter = 100} and \code{iter.tol = 0.01} are employed.



\subsection{Starting values}
If numeric optimization techniques are used, the starting for all parameters are drawn from the uniform distribution on an interval [-1, 1] by the following commands (set as default):
\begin{verbatim}
start.val.lo = -1, start.val.up = 1, seed.input = 42
\end{verbatim}
As usual, the \code{seed.input} ensures reproducibility.
The starting values can be varied by the user via arguments \code{start.val.lo} and \code{start.val.up} and setting \code{custom.start.val} to TRUE.











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Concluding remarks}
\label{sec:conclusion}

The \proglang{R}-package \pkg{pdynmc} provides a function to estimate linear dynamic panel data models. The implementation allows for general lag structures of the explanatory variables, which may encompass lags of the dependent variable and further non-lagged dependent explanatory variables. For estimation, linear and nonlinear moment conditions are derived from the model assumptions; further controls and external instruments (if available) may also be added. Estimation is carried out by numerical optimization of the GMM objective function. Corresponding closed form solutions are computed -- where possible -- and stored besides the results from numerical optimization. The estimation routine can handle balanced and unbalanced panel data sets and provides one-step-, two-step-, and continuously updating estimation. Accounting for (unobserved) time-specific effects is possible by including time dummies; alternatively, both sides of the equation can be transformed such that the time-specific heterogeneity is partialled out. The partialling out option is experimental at the moment. We plan to investigate the effects and implications of this way of dealing with unobserved time-specific heterogeneity in greater detail in the future. Different choices for the weighting matrix, which guides the aggregation of moment conditions in one-step GMM estimation are available. Concerning the computation of standard errors for the coefficient estimates, the following options are currently available in \pkg{pdynmc}: non-robust one- and two-step standard errors and robust one-step- and Windmeijer-corrected two-step standard errors. Some standard hypothesis and specification tests are also available. Among them are Wald tests, overidentifying restrictions tests and a test for serial correlation in the idiosyncratic remainder components.

%At the current stage of the project the package still requires speed improvements. As continuously updating GMM estimation may carry out the estimation procedure many times, this option for estimation is not yet available.

We plan to extend the package by additional features in the future.
Next steps involve:
\begin{itemize}
    \item Incorporate further diagnostics and tests to assess the validity of the estimated specifications and the underlying moment conditions and assumptions (e.g., testing the `constant correlated effects' assumption and testing for structural breaks).
    \item Add computation of confidence and prediction intervals.
%    \item Facilitate choosing an adequate dynamic specification by lag selection techniques.
%    \item Include moment selection capabilities based on an appropriate criterion into GMM estimation which allow to remove certain instruments/moment conditions.
    \item Expand the possible choices for the one-step weighting matrix by, e.g., the proposition in \citet{Kiv2007WP} for GMM estimation based on linear HNR- and AB moment conditions.
%    \item Allow further types of moment conditions; an example are moment conditions derived from assumptions about second (alternatively, or additionally: third, fourth, \dots) moments of the $y_{i,t}$ process \citep[e.g., homoscedasticity as mentioned in][]{AhnSch1995}
%    \item Enable the user to choose time period $T$ instead of $t$ as reference period for all moment conditions.
    \item Implement the IV estimator solely based on the nonlinear moment conditions proposed by \citet{PuaFriSch2019a,PuaFriSch2019b}.
\end{itemize}












%\clearpage
%\section*{References}
\bibliography{REFERENCES}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\section*{Appendix}



\end{document}
